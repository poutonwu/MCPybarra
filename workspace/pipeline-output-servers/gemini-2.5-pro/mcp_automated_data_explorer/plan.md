# MCP Server Implementation Plan: Automated Data Analysis

### **Server Overview**

This server provides automated data exploration and analysis capabilities. It allows users to load CSV datasets into memory, execute custom Python analysis scripts on them, and generate comprehensive exploratory data analysis reports with visualizations. The server is designed to be a powerful backend for data science workflows, enabling dynamic interaction with multiple datasets.

### **File to be Generated**

-   `mcp_data_analyzer.py`

### **Dependencies**

-   `mcp`
-   `pandas`
-   `numpy`
-   `scipy`
-   `scikit-learn`
-   `statsmodels`
-   `ydata-profiling`

### **MCP Tools Plan**

---

#### **1. Tool: `load_csv`**

*   **Function Name**: `load_csv`
*   **Description**: Loads a dataset from a specified local CSV file into memory and assigns it a unique name for future reference. This allows for the management of multiple datasets simultaneously.
*   **Parameters**:
    *   `dataset_name` (str): A unique identifier for the dataset. If a dataset with this name already exists, it will be overwritten.
    *   `file_path` (str): The absolute or relative path to the CSV file to be loaded.
*   **Return Value**:
    *   (dict): A dictionary confirming the successful loading of the data, including the number of rows and columns.
    *   Example: `{"status": "success", "dataset_name": "titanic", "rows": 891, "columns": 12}`
*   **Error Handling**: Raises `FileNotFoundError` if the path is invalid or `Exception` for parsing errors.

---

#### **2. Tool: `run_script`**

*   **Function Name**: `run_script`
*   **Description**: Executes a provided Python script string. The script has access to all loaded datasets via a global dictionary named `DATASETS`. It can use pre-imported libraries (`pandas`, `numpy`, `scipy`, `sklearn`, `statsmodels`) to perform complex data manipulations, statistical analysis, or modeling. Results can be stored back into the `DATASETS` dictionary for subsequent operations.
*   **Parameters**:
    *   `script_content` (str): A string containing the Python code to execute. The code can interact with a dictionary called `DATASETS` which holds all loaded dataframes.
*   **Return Value**:
    *   (dict): A dictionary containing the standard output (`stdout`) and standard error (`stderr`) generated by the script, along with a final status message.
    *   Example: `{"status": "success", "stdout": "Analysis complete. New feature 'age_group' added to 'titanic' dataset.", "stderr": ""}`
*   **Error Handling**: Captures and returns any exceptions raised during script execution in the `stderr` field of the return dictionary.

---

#### **3. Tool: `explore_data`**

*   **Function Name**: `explore_data`
*   **Description**: Performs a comprehensive exploratory data analysis (EDA) on a specified loaded dataset using the `ydata-profiling` library. It automatically analyzes the data's structure, statistics, correlations, and missing values, and generates a detailed report in JSON format.
*   **Parameters**:
    *   `dataset_name` (str): The unique identifier of the dataset to analyze.
*   **Return Value**:
    *   (str): A JSON string containing the full `ydata-profiling` report. This JSON can be used by a client application to render interactive visualizations and tables.
*   **Error Handling**: Raises `ValueError` if the specified `dataset_name` does not exist in memory.