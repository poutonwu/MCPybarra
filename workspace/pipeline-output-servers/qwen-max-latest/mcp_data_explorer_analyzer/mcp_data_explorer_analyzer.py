import sys
import json
import os
import pandas as pd
import numpy as np
from scipy import stats
from sklearn import preprocessing
import matplotlib.pyplot as plt
import seaborn as sns
from mcp.server.fastmcp import FastMCP

# Initialize the FastMCP server
mcp = FastMCP("mcp_data_explorer")

# Internal dictionary to store datasets in memory
datasets = {}

@mcp.tool()
def load_csv(file_path: str, dataset_id: str) -> str:
    """
    Loads a CSV file into memory as a pandas DataFrame and stores it in an internal dictionary for subsequent operations.

    Args:
        file_path (str): The path to the CSV file to be loaded.
        dataset_id (str): A unique identifier for the dataset being loaded.

    Returns:
        str: A confirmation message indicating successful loading of the dataset.

    Example:
        load_csv(file_path="data/sample.csv", dataset_id="dataset1")
    """
    try:
        # Validate parameters
        if not file_path or not isinstance(file_path, str):
            raise ValueError("file_path must be a non-empty string.")
        if not dataset_id or not isinstance(dataset_id, str):
            raise ValueError("dataset_id must be a non-empty string.")

        # Load the CSV file into a pandas DataFrame
        df = pd.read_csv(file_path)

        # Store the DataFrame in the internal dictionary
        datasets[dataset_id] = df

        # Return a confirmation message
        return json.dumps({"message": f"Dataset '{dataset_id}' successfully loaded."})

    except FileNotFoundError as e:
        return json.dumps({"error": f"File not found: {str(e)}"})
    except Exception as e:
        # Handle exceptions and return an error message
        return json.dumps({"error": str(e)})

@mcp.tool()
def run_script(script_code: str, input_datasets: list, output_dataset_id: str) -> str:
    """
    Executes a user-provided Python script that performs data analysis using libraries such as pandas, numpy, scipy, sklearn,
    and statsmodels. The processed results are stored in memory for further use.

    Args:
        script_code (str): The Python code to execute.
        input_datasets (list of str): List of dataset IDs required for the script execution.
        output_dataset_id (str): Identifier for the output dataset generated by the script.

    Returns:
        str: A confirmation message indicating successful execution and storage of the output dataset.

    Example:
        run_script(script_code="df['new_column'] = df['existing_column'] * 2", 
                   input_datasets=["dataset1"], output_dataset_id="processed_dataset")
    """
    try:
        # Validate parameters
        if not script_code or not isinstance(script_code, str):
            raise ValueError("script_code must be a non-empty string.")
        if not input_datasets or not isinstance(input_datasets, list):
            raise ValueError("input_datasets must be a non-empty list of strings.")
        if not output_dataset_id or not isinstance(output_dataset_id, str):
            raise ValueError("output_dataset_id must be a non-empty string.")

        # Prepare local variables for script execution
        local_vars = {}
        for dataset_id in input_datasets:
            if dataset_id not in datasets:
                raise ValueError(f"Dataset '{dataset_id}' not found in memory.")
            local_vars[dataset_id] = datasets[dataset_id]

        # Execute the provided script code
        exec(script_code, globals(), local_vars)

        # Assume the script creates a variable named 'output' for the result
        if 'output' not in local_vars:
            raise ValueError("The script must define a variable named 'output'.")

        # Store the output dataset in memory
        datasets[output_dataset_id] = local_vars['output']

        # Return a confirmation message
        return json.dumps({"message": f"Script executed successfully. Output stored with ID '{output_dataset_id}'."})

    except Exception as e:
        # Handle exceptions and return an error message
        return json.dumps({"error": str(e)})

@mcp.tool()
def explore_data(dataset_ids: list, exploration_id: str) -> str:
    """
    Automatically analyzes the structure of one or more datasets, generates an exploration plan, performs insightful data
    visualizations, and saves the results back into memory.

    Args:
        dataset_ids (list of str): List of dataset IDs to explore.
        exploration_id (str): Identifier for storing the exploration results.

    Returns:
        str: A summary of the exploration insights and visualization URLs as applicable.

    Example:
        explore_data(dataset_ids=["dataset1", "dataset2"], exploration_id="exploration_results")
    """
    try:
        # Validate parameters
        if not dataset_ids or not isinstance(dataset_ids, list):
            raise ValueError("dataset_ids must be a non-empty list of strings.")
        if not exploration_id or not isinstance(exploration_id, str):
            raise ValueError("exploration_id must be a non-empty string.")

        exploration_results = {}

        # Ensure the plots directory exists
        os.makedirs("workspace/plots", exist_ok=True)

        for dataset_id in dataset_ids:
            if dataset_id not in datasets:
                raise ValueError(f"Dataset '{dataset_id}' not found in memory.")

            df = datasets[dataset_id]

            # Generate basic statistics
            exploration_results[dataset_id] = {
                "summary": df.describe().to_dict(),
                "visualizations": []
            }

            # Generate visualizations
            for column in df.select_dtypes(include=[np.number]).columns.tolist():
                plt.figure(figsize=(8, 4))
                sns.histplot(df[column].dropna(), kde=True)
                plt.title(f"Histogram and KDE for {column}")
                plot_path = f"workspace/plots/{exploration_id}_{dataset_id}_{column}.png"
                plt.savefig(plot_path)
                plt.close()

                exploration_results[dataset_id]["visualizations"].append(plot_path)

        # Store the exploration results in memory
        datasets[exploration_id] = exploration_results

        # Return a summary of the exploration insights
        return json.dumps({
            "message": "Exploration completed successfully.",
            "results": exploration_results
        })

    except Exception as e:
        # Handle exceptions and return an error message
        return json.dumps({"error": str(e)})

if __name__ == "__main__":
    sys.stdout.reconfigure(encoding='utf-8')
    mcp.run()