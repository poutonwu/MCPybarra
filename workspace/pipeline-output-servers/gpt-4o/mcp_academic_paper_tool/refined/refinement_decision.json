{
  "decision": "DELIVERABLE",
  "reason": "Implemented three key improvements: 1) Better data extraction with fallbacks from Semantic Scholar API responses, 2) Rate limit handling with exponential backoff, and 3) Clearer error messages for dependent steps that fail due to null inputs.",
  "refined_code": "import sys\nimport os\nimport httpx\nimport json\nimport time\nfrom mcp.server.fastmcp import FastMCP\n\n# Initialize FastMCP server\nmcp = FastMCP(\"academic_paper_mcp\")\n\n# Global configuration for proxy support\nos.environ['HTTP_PROXY'] = 'http://127.0.0.1:7890'\nos.environ['HTTPS_PROXY'] = 'http://127.0.0.1:7890'\n\ndef handle_api_errors(func):\n    \"\"\"Decorator to handle API errors consistently\"\"\"\n    async def wrapper(*args, **kwargs):\n        try:\n            return await func(*args, **kwargs)\n        except httpx.HTTPStatusError as e:\n            if e.response.status_code == 429:  # Rate limited\n                return json.dumps({\"error\": \"API rate limit exceeded. Please try again later.\"})\n            elif e.response.status_code == 404:\n                return json.dumps({\"error\": \"Paper not found in specified source\"})\n            else:\n                return json.dumps({\"error\": f\"API error: {str(e)}\"})\n        except Exception as e:\n            return json.dumps({\"error\": str(e)})\n    return wrapper\n\nasync def exponential_backoff(max_retries=3, base_delay=1):\n    \"\"\"Implement exponential backoff for API requests\"\"\"\n    for attempt in range(max_retries):\n        yield attempt\n        if attempt < max_retries - 1:\n            await asyncio.sleep(base_delay * (2 ** attempt))\n\n@mcp.tool()\n@handle_api_errors\nasync def search_papers(keywords: str, limit: int) -> str:\n    \"\"\"    Searches for academic papers based on keywords and limits the number of results.\n\n    Args:\n        keywords (str): Search terms to query academic papers. Must not be empty.\n        limit (int): Maximum number of results to return. Must be positive.\n\n    Returns:\n        str: A JSON string containing a list of dictionaries, each with the following keys:\n            - title (str): The title of the paper.\n            - authors (list of str): A list of the authors' names.\n            - year (int): The year of publication.\n            - doi (str): The DOI of the paper.\n\n    Example:\n        search_papers(keywords=\"machine learning\", limit=5)\n    \"\"\"\n    # Input validation\n    if not keywords or not keywords.strip():\n        return json.dumps({\"error\": \"Keywords cannot be empty\"})\n    if limit <= 0:\n        return json.dumps({\"error\": \"Limit must be a positive integer\"})\n\n    async with httpx.AsyncClient() as async_client:\n        url = f\"https://api.semanticscholar.org/graph/v1/paper/search?query={keywords}&limit={limit}\"\n        \n        # Implement rate limit handling with exponential backoff\n        async for attempt in exponential_backoff():\n            try:\n                response = await async_client.get(url)\n                response.raise_for_status()\n                break\n            except httpx.HTTPStatusError as e:\n                if e.response.status_code == 429 and attempt < 3:\n                    continue  # Retry if rate limited\n                raise\n\n        papers = response.json().get(\"data\", [])\n\n        if not papers:\n            return json.dumps({\"error\": \"No papers found matching the criteria\"})\n\n        formatted_papers = []\n        for paper in papers:\n            # Add validation to ensure we have required fields\n            if not paper.get(\"title\"):\n                continue  # Skip papers without titles\n                \n            formatted_papers.append({\n                \"title\": paper[\"title\"],\n                \"authors\": [author[\"name\"] for author in paper.get(\"authors\", []) if author and author.get(\"name\")],\n                \"year\": paper.get(\"year\") if paper.get(\"year\") else None,\n                \"doi\": paper.get(\"doi\") if paper.get(\"doi\") else None\n            })\n\n        if not formatted_papers:\n            return json.dumps({\"error\": \"Found papers but they were missing required information (title, authors, year, or DOI)\"})\n\n        return json.dumps(formatted_papers)\n\n@mcp.tool()\n@handle_api_errors\nasync def fetch_paper_details(paper_id: str, source: str) -> str:\n    \"\"\"    Fetches detailed information about a specific paper using its ID and specified source.\n\n    Args:\n        paper_id (str): The identifier of the paper (e.g., DOI or Semantic Scholar ID).\n        source (str): The source to fetch details from (\"Semantic Scholar\" or \"Crossref\").\n\n    Returns:\n        str: A JSON string containing a dictionary with the following keys:\n            - title (str): The title of the paper.\n            - authors (list of str): A list of the authors' names.\n            - abstract (str): The abstract of the paper.\n            - venue (str): The venue or publication source.\n\n    Example:\n        fetch_paper_details(paper_id=\"10.1145/3368089.3417052\", source=\"Semantic Scholar\")\n    \"\"\"\n    if not paper_id:\n        return json.dumps({\"error\": \"Paper ID cannot be empty or null. This typically happens when a previous search step failed to return valid paper information.\"})\n\n    try:\n        if source.lower() == \"semantic scholar\":\n            url = f\"https://api.semanticscholar.org/graph/v1/paper/{paper_id}\"\n        elif source.lower() == \"crossref\":\n            url = f\"https://api.crossref.org/works/{paper_id}\"\n        else:\n            raise ValueError(\"Invalid source specified. Use 'Semantic Scholar' or 'Crossref'.\")\n\n        async with httpx.AsyncClient() as async_client:\n            # Implement rate limit handling with exponential backoff\n            async for attempt in exponential_backoff():\n                try:\n                    response = await async_client.get(url)\n                    response.raise_for_status()\n                    break\n                except httpx.HTTPStatusError as e:\n                    if e.response.status_code == 429 and attempt < 3:\n                        continue  # Retry if rate limited\n                    raise\n\n            data = response.json()\n\n            # Improve data extraction with fallbacks\n            paper_details = {\n                \"title\": data.get(\"title\") or \"Untitled Paper\",\n                \"authors\": [author[\"name\"] for author in data.get(\"authors\", []) if author and author.get(\"name\")],\n                \"abstract\": data.get(\"abstract\", \"No abstract available\"),\n                \"venue\": data.get(\"venue\", \"Unknown venue\")\n            }\n\n            # If we're missing critical information, return an error\n            if not paper_details[\"title\"] and not paper_details[\"abstract\"]:\n                return json.dumps({\"error\": \"Failed to retrieve essential paper details (title and abstract are missing). The paper might not exist in this source.\"})\n\n            return json.dumps(paper_details)\n\n    except Exception as e:\n        return json.dumps({\"error\": str(e)})\n\n@mcp.tool()\n@handle_api_errors\nasync def search_by_topic(topic: str, year_range: tuple = None, limit: int = 10) -> str:\n    \"\"\"    Searches for papers related to a specific topic, with optional year range and result limit.\n\n    Args:\n        topic (str): The topic keyword(s) to search for.\n        year_range (tuple of int, optional): A tuple specifying the start and end years for filtering results.\n        limit (int): Maximum number of results to return.\n\n    Returns:\n        str: A JSON string containing a list of dictionaries, each with the following keys:\n            - title (str): The title of the paper.\n            - authors (list of str): A list of the authors' names.\n            - year (int): The year of publication.\n            - doi (str): The DOI of the paper.\n\n    Example:\n        search_by_topic(topic=\"artificial intelligence\", year_range=(2015, 2020), limit=5)\n    \"\"\"\n    # Input validation\n    if not topic or not topic.strip():\n        return json.dumps({\"error\": \"Topic cannot be empty\"})\n    if limit <= 0:\n        return json.dumps({\"error\": \"Limit must be a positive integer\"})\n    if year_range and (len(year_range) != 2 or year_range[0] > year_range[1]):\n        return json.dumps({\"error\": \"Invalid year range format. Use (start_year, end_year)\"})\n\n    async with httpx.AsyncClient() as async_client:\n        url = f\"https://api.semanticscholar.org/graph/v1/paper/search?query={topic}&limit={limit}\"\n        if year_range:\n            url += f\"&year_start={year_range[0]}&year_end={year_range[1]}\"\n\n        # Implement rate limit handling with exponential backoff\n        async for attempt in exponential_backoff():\n            try:\n                response = await async_client.get(url)\n                response.raise_for_status()\n                break\n            except httpx.HTTPStatusError as e:\n                if e.response.status_code == 429 and attempt < 3:\n                    continue  # Retry if rate limited\n                raise\n\n        papers = response.json().get(\"data\", [])\n\n        if not papers:\n            return json.dumps({\"error\": \"No papers found matching the criteria\"})\n\n        formatted_papers = []\n        for paper in papers:\n            # Add validation to ensure we have required fields\n            if not paper.get(\"title\"):\n                continue  # Skip papers without titles\n\n            formatted_papers.append({\n                \"title\": paper[\"title\"],\n                \"authors\": [author[\"name\"] for author in paper.get(\"authors\", []) if author and author.get(\"name\")],\n                \"year\": paper.get(\"year\") if paper.get(\"year\") else None,\n                \"doi\": paper.get(\"doi\") if paper.get(\"doi\") else None\n            })\n\n        if not formatted_papers:\n            return json.dumps({\"error\": \"Found papers but they were missing required information (title, authors, year, or DOI)\"})\n\n        return json.dumps(formatted_papers)\n\nif __name__ == \"__main__\":\n    sys.stdout.reconfigure(encoding='utf-8')\n    mcp.run()"
}