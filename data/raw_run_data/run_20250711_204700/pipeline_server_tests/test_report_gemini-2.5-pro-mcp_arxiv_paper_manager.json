{
  "server_name": "server",
  "parent_dir": "refined",
  "report_name": "gemini-2.5-pro-mcp_arxiv_paper_manager",
  "server_path": "workspace/pipeline-output-servers/gemini-2.5-pro/mcp_arxiv_paper_manager/refined/server.py",
  "timestamp": "2025-07-11T20:47:03.591731",
  "tools": [
    {
      "name": "search_papers",
      "description": "\n    Searches the arXiv repository for papers matching a given query.\n\n    This function queries the arXiv API for academic papers based on a search\n    term. It can search by keywords, author names, or other arXiv-supported\n    syntax. The results are sorted by their submission date.\n\n    Args:\n        query (str): The search query (e.g., \"quantum computing\", \"author:John Doe\").\n        max_results (int): The maximum number of results to return. Defaults to 10.\n\n    Returns:\n        str: A JSON string representing a list of papers with their metadata,\n             including ID, title, authors, and summary. Returns a JSON object\n             with an \"error\" key if the search fails.\n    ",
      "args_schema": {
        "properties": {
          "query": {
            "title": "Query",
            "type": "string"
          },
          "max_results": {
            "default": 10,
            "title": "Max Results",
            "type": "integer"
          }
        },
        "required": [
          "query"
        ],
        "title": "search_papersArguments",
        "type": "object"
      }
    },
    {
      "name": "download_paper",
      "description": "\n    Downloads the PDF of a specific paper using its arXiv ID.\n\n    This function fetches a paper's metadata from arXiv using its unique ID,\n    retrieves the PDF URL, and downloads the file into a local 'papers'\n    directory. The downloaded file is named using its arXiv ID (e.g., '2301.12345.pdf').\n\n    Args:\n        paper_id (str): The unique arXiv ID of the paper to download (e.g., '2301.12345').\n\n    Returns:\n        str: A JSON string indicating the result of the download operation.\n             On success, it returns a message confirming the download. On failure,\n             it returns a JSON object with an \"error\" key.\n    ",
      "args_schema": {
        "properties": {
          "paper_id": {
            "title": "Paper Id",
            "type": "string"
          }
        },
        "required": [
          "paper_id"
        ],
        "title": "download_paperArguments",
        "type": "object"
      }
    },
    {
      "name": "list_papers",
      "description": "\n    Lists all the academic papers that have been downloaded locally.\n\n    This function scans the 'papers' directory and returns a list of all\n    files that end with the '.pdf' extension. This provides a quick way to see\n    which papers are available for local reading.\n\n    Returns:\n        str: A JSON string representing a list of filenames of downloaded papers.\n             If the directory doesn't exist or is empty, it returns an empty list.\n             Returns a JSON object with an \"error\" key if an unexpected error occurs.\n    ",
      "args_schema": {
        "properties": {},
        "title": "list_papersArguments",
        "type": "object"
      }
    },
    {
      "name": "read_paper",
      "description": "\n    Reads the text content from a specified, locally stored PDF file.\n\n    This function opens a PDF from the local 'papers' directory and uses the\n    PyMuPDF library (fitz) to extract all text content from its pages.\n    The extracted text is returned as a single string.\n\n    Args:\n        filename (str): The filename of the paper to read from the 'papers'\n                        directory (e.g., '2301.12345.pdf').\n\n    Returns:\n        str: A JSON string containing the filename and the full extracted text\n             of the paper. If the file is not found or an error occurs during\n             reading, it returns a JSON object with an \"error\" key.\n    ",
      "args_schema": {
        "properties": {
          "filename": {
            "title": "Filename",
            "type": "string"
          }
        },
        "required": [
          "filename"
        ],
        "title": "read_paperArguments",
        "type": "object"
      }
    }
  ],
  "test_results": {
    "search_papers": [
      {
        "case_name": "Basic Search with Default Results",
        "purpose": "验证使用默认max_results参数值的基本搜索功能",
        "args": {
          "query": "quantum computing"
        },
        "response": {
          "result": "[{\"id\": \"2507.08000v1\", \"title\": \"Impact of Pretraining Word Co-occurrence on Compositional Generalization in Multimodal Models\", \"authors\": [\"Helen Qu\", \"Sang Michael Xie\"], \"summary\": \"CLIP and large multimodal models (LMMs) have better accuracy on examples\\ninvolving concepts that are highly represented in the training data. However,\\nthe role of concept combinations in the training data on compositional\\ngeneralization is largely unclear -- for instance, how does accuracy vary when\\na common object appears in an uncommon pairing with another object? In this\\npaper, we investigate how word co-occurrence statistics in the pretraining\\ndataset (a proxy for co-occurrence of visual concepts) impacts CLIP/LMM\\nperformance. To disentangle the effects of word co-occurrence frequencies from\\nsingle-word frequencies, we measure co-occurrence with pointwise mutual\\ninformation (PMI), which normalizes the joint probability of two words\\nco-occurring by the probability of co-occurring independently. Using\\nsynthetically generated images with a variety of concept pairs, we show a\\nstrong correlation between PMI in the CLIP pretraining data and zero-shot\\naccuracy in CLIP models trained on LAION-400M (r=0.97 and 14% accuracy gap\\nbetween images in the top and bottom 5% of PMI values), demonstrating that even\\naccuracy on common concepts is affected by the combination of concepts in the\\nimage. Leveraging this finding, we reproduce this effect in natural images by\\nediting them to contain pairs with varying PMI, resulting in a correlation of\\nr=0.75. Finally, we demonstrate that this behavior in CLIP transfers to LMMs\\nbuilt on top of CLIP (r=0.70 for TextVQA, r=0.62 for VQAv2). Our findings\\nhighlight the need for algorithms and architectures that improve compositional\\ngeneralization in multimodal models without scaling the training data\\ncombinatorially. Our code is available at\\nhttps://github.com/helenqu/multimodal-pretraining-pmi.\"}, {\"id\": \"2507.07999v1\", \"title\": \"Traceable Evidence Enhanced Visual Grounded Reasoning: Evaluation and Methodology\", \"authors\": [\"Haochen Wang\", \"Xiangtai Li\", \"Zilong Huang\", \"Anran Wang\", \"Jiacong Wang\", \"Tao Zhang\", \"Jiani Zheng\", \"Sule Bai\", \"Zijian Kang\", \"Jiashi Feng\", \"Zhuochen Wang\", \"Zhaoxiang Zhang\"], \"summary\": \"Models like OpenAI-o3 pioneer visual grounded reasoning by dynamically\\nreferencing visual regions, just like human \\\"thinking with images\\\". However, no\\nbenchmark exists to evaluate these capabilities holistically. To bridge this\\ngap, we propose TreeBench (Traceable Evidence Evaluation Benchmark), a\\ndiagnostic benchmark built on three principles: (1) focused visual perception\\nof subtle targets in complex scenes, (2) traceable evidence via bounding box\\nevaluation, and (3) second-order reasoning to test object interactions and\\nspatial hierarchies beyond simple object localization. Prioritizing images with\\ndense objects, we initially sample 1K high-quality images from SA-1B, and\\nincorporate eight LMM experts to manually annotate questions, candidate\\noptions, and answers for each image. After three stages of quality control,\\nTreeBench consists of 405 challenging visual question-answering pairs, even the\\nmost advanced models struggle with this benchmark, where none of them reach 60%\\naccuracy, e.g., OpenAI-o3 scores only 54.87. Furthermore, we introduce TreeVGR\\n(Traceable Evidence Enhanced Visual Grounded Reasoning), a training paradigm to\\nsupervise localization and reasoning jointly with reinforcement learning,\\nenabling accurate localizations and explainable reasoning pathways. Initialized\\nfrom Qwen2.5-VL-7B, it improves V* Bench (+16.8), MME-RealWorld (+12.6), and\\nTreeBench (+13.4), proving traceability is key to advancing vision-grounded\\nreasoning. The code is available at https://github.com/Haochen-Wang409/TreeVGR.\"}, {\"id\": \"2507.07998v1\", \"title\": \"PyVision: Agentic Vision with Dynamic Tooling\", \"authors\": [\"Shitian Zhao\", \"Haoquan Zhang\", \"Shaoheng Lin\", \"Ming Li\", \"Qilong Wu\", \"Kaipeng Zhang\", \"Chen Wei\"], \"summary\": \"LLMs are increasingly deployed as agents, systems capable of planning,\\nreasoning, and dynamically calling external tools. However, in visual\\nreasoning, prior approaches largely remain limited by predefined workflows and\\nstatic toolsets. In this report, we present PyVision, an interactive,\\nmulti-turn framework that enables MLLMs to autonomously generate, execute, and\\nrefine Python-based tools tailored to the task at hand, unlocking flexible and\\ninterpretable problem-solving. We develop a taxonomy of the tools created by\\nPyVision and analyze their usage across a diverse set of benchmarks.\\nQuantitatively, PyVision achieves consistent performance gains, boosting\\nGPT-4.1 by +7.8% on V* and Claude-4.0-Sonnet by +31.1% on VLMsAreBlind-mini.\\nThese results point to a broader shift: dynamic tooling allows models not just\\nto use tools, but to invent them, advancing toward more agentic visual\\nreasoning.\"}, {\"id\": \"2507.07997v1\", \"title\": \"MGVQ: Could VQ-VAE Beat VAE? A Generalizable Tokenizer with Multi-group Quantization\", \"authors\": [\"Mingkai Jia\", \"Wei Yin\", \"Xiaotao Hu\", \"Jiaxin Guo\", \"Xiaoyang Guo\", \"Qian Zhang\", \"Xiao-Xiao Long\", \"Ping Tan\"], \"summary\": \"Vector Quantized Variational Autoencoders (VQ-VAEs) are fundamental models\\nthat compress continuous visual data into discrete tokens. Existing methods\\nhave tried to improve the quantization strategy for better reconstruction\\nquality, however, there still exists a large gap between VQ-VAEs and VAEs. To\\nnarrow this gap, we propose \\\\NickName, a novel method to augment the\\nrepresentation capability of discrete codebooks, facilitating easier\\noptimization for codebooks and minimizing information loss, thereby enhancing\\nreconstruction quality. Specifically, we propose to retain the latent dimension\\nto preserve encoded features and incorporate a set of sub-codebooks for\\nquantization. Furthermore, we construct comprehensive zero-shot benchmarks\\nfeaturin...[截断]，共计14968字符，剩余8968字符"
        },
        "execution_time": 10.613721132278442,
        "is_functional_test": true
      },
      {
        "case_name": "Search by Author",
        "purpose": "验证通过作者名称进行论文搜索的功能",
        "args": {
          "query": "author:Einstein",
          "max_results": 5
        },
        "response": {
          "result": "[]"
        },
        "execution_time": 1.433729648590088,
        "is_functional_test": true
      },
      {
        "case_name": "Keyword Search with Limited Results",
        "purpose": "验证指定关键词和限制返回结果数量的搜索功能",
        "args": {
          "query": "machine learning",
          "max_results": 3
        },
        "response": {
          "result": "[{\"id\": \"2507.08000v1\", \"title\": \"Impact of Pretraining Word Co-occurrence on Compositional Generalization in Multimodal Models\", \"authors\": [\"Helen Qu\", \"Sang Michael Xie\"], \"summary\": \"CLIP and large multimodal models (LMMs) have better accuracy on examples\\ninvolving concepts that are highly represented in the training data. However,\\nthe role of concept combinations in the training data on compositional\\ngeneralization is largely unclear -- for instance, how does accuracy vary when\\na common object appears in an uncommon pairing with another object? In this\\npaper, we investigate how word co-occurrence statistics in the pretraining\\ndataset (a proxy for co-occurrence of visual concepts) impacts CLIP/LMM\\nperformance. To disentangle the effects of word co-occurrence frequencies from\\nsingle-word frequencies, we measure co-occurrence with pointwise mutual\\ninformation (PMI), which normalizes the joint probability of two words\\nco-occurring by the probability of co-occurring independently. Using\\nsynthetically generated images with a variety of concept pairs, we show a\\nstrong correlation between PMI in the CLIP pretraining data and zero-shot\\naccuracy in CLIP models trained on LAION-400M (r=0.97 and 14% accuracy gap\\nbetween images in the top and bottom 5% of PMI values), demonstrating that even\\naccuracy on common concepts is affected by the combination of concepts in the\\nimage. Leveraging this finding, we reproduce this effect in natural images by\\nediting them to contain pairs with varying PMI, resulting in a correlation of\\nr=0.75. Finally, we demonstrate that this behavior in CLIP transfers to LMMs\\nbuilt on top of CLIP (r=0.70 for TextVQA, r=0.62 for VQAv2). Our findings\\nhighlight the need for algorithms and architectures that improve compositional\\ngeneralization in multimodal models without scaling the training data\\ncombinatorially. Our code is available at\\nhttps://github.com/helenqu/multimodal-pretraining-pmi.\"}, {\"id\": \"2507.07999v1\", \"title\": \"Traceable Evidence Enhanced Visual Grounded Reasoning: Evaluation and Methodology\", \"authors\": [\"Haochen Wang\", \"Xiangtai Li\", \"Zilong Huang\", \"Anran Wang\", \"Jiacong Wang\", \"Tao Zhang\", \"Jiani Zheng\", \"Sule Bai\", \"Zijian Kang\", \"Jiashi Feng\", \"Zhuochen Wang\", \"Zhaoxiang Zhang\"], \"summary\": \"Models like OpenAI-o3 pioneer visual grounded reasoning by dynamically\\nreferencing visual regions, just like human \\\"thinking with images\\\". However, no\\nbenchmark exists to evaluate these capabilities holistically. To bridge this\\ngap, we propose TreeBench (Traceable Evidence Evaluation Benchmark), a\\ndiagnostic benchmark built on three principles: (1) focused visual perception\\nof subtle targets in complex scenes, (2) traceable evidence via bounding box\\nevaluation, and (3) second-order reasoning to test object interactions and\\nspatial hierarchies beyond simple object localization. Prioritizing images with\\ndense objects, we initially sample 1K high-quality images from SA-1B, and\\nincorporate eight LMM experts to manually annotate questions, candidate\\noptions, and answers for each image. After three stages of quality control,\\nTreeBench consists of 405 challenging visual question-answering pairs, even the\\nmost advanced models struggle with this benchmark, where none of them reach 60%\\naccuracy, e.g., OpenAI-o3 scores only 54.87. Furthermore, we introduce TreeVGR\\n(Traceable Evidence Enhanced Visual Grounded Reasoning), a training paradigm to\\nsupervise localization and reasoning jointly with reinforcement learning,\\nenabling accurate localizations and explainable reasoning pathways. Initialized\\nfrom Qwen2.5-VL-7B, it improves V* Bench (+16.8), MME-RealWorld (+12.6), and\\nTreeBench (+13.4), proving traceability is key to advancing vision-grounded\\nreasoning. The code is available at https://github.com/Haochen-Wang409/TreeVGR.\"}, {\"id\": \"2507.07995v1\", \"title\": \"Single-pass Adaptive Image Tokenization for Minimum Program Search\", \"authors\": [\"Shivam Duggal\", \"Sanghyun Byun\", \"William T. Freeman\", \"Antonio Torralba\", \"Phillip Isola\"], \"summary\": \"According to Algorithmic Information Theory (AIT) -- Intelligent\\nrepresentations compress data into the shortest possible program that can\\nreconstruct its content, exhibiting low Kolmogorov Complexity (KC). In\\ncontrast, most visual representation learning systems use fixed-length\\nrepresentations for all inputs, ignoring variations in complexity or\\nfamiliarity. Recent adaptive tokenization methods address this by allocating\\nvariable-length representations but typically require test-time search over\\nmultiple encodings to find the most predictive one. Inspired by Kolmogorov\\nComplexity principles, we propose a single-pass adaptive tokenizer, KARL, which\\npredicts the appropriate number of tokens for an image in a single forward\\npass, halting once its approximate KC is reached. The token count serves as a\\nproxy for the minimum description length. KARL's training procedure closely\\nresembles the Upside-Down Reinforcement Learning paradigm, as it learns to\\nconditionally predict token halting based on a desired reconstruction quality.\\nKARL matches the performance of recent adaptive tokenizers while operating in a\\nsingle pass. We present scaling laws for KARL, analyzing the role of\\nencoder/decoder size, continuous vs. discrete tokenization and more.\\nAdditionally, we offer a conceptual study drawing an analogy between Adaptive\\nImage Tokenization and Algorithmic Information Theory, examining the predicted\\nimage complexity (KC) across axes such as structure vs. noise and in- vs.\\nout-of-distribution familiarity -- revealing alignment with human intuition.\"}]"
        },
        "execution_time": 1.9042081832885742,
        "is_functional_test": true
      },
      {
        "case_name": "Empty Query Test",
        "purpose": "验证空查询字符串是否被正确处理",
        "args": {
          "query": "",
          "max_results": 10
        },
        "response": {
          "result": "[]"
        },
        "execution_time": 1.5490782260894775,
        "is_functional_test": false
      },
      {
        "case_name": "Invalid Max Results Value",
        "purpose": "验证非正整数max_results参数的错误处理",
        "args": {
          "query": "blockchain",
          "max_results": -5
        },
        "response": {
          "result": "[]"
        },
        "execution_time": 0.004515171051025391,
        "is_functional_test": false
      },
      {
        "case_name": "Special Characters in Query",
        "purpose": "验证包含特殊字符的查询是否能被安全处理",
        "args": {
          "query": "neural networks!@#$%^&*()"
        },
        "response": {
          "result": "[]"
        },
        "execution_time": 1.712235689163208,
        "is_functional_test": true
      },
      {
        "case_name": "Maximum Allowed Results",
        "purpose": "验证arXiv API允许的最大结果数（通常为300）",
        "args": {
          "query": "AI ethics",
          "max_results": 300
        },
        "response": {
          "result": "{\"error\": \"An error occurred during search: Page of results was unexpectedly empty (https://export.arxiv.org/api/query?search_query=AI+ethics&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100)\"}"
        },
        "execution_time": 15.023674726486206,
        "is_functional_test": true
      },
      {
        "case_name": "Non-Existent Topic Search",
        "purpose": "验证无匹配结果时工具的行为",
        "args": {
          "query": "this topic does not exist",
          "max_results": 10
        },
        "response": {
          "result": "[{\"id\": \"2507.08000v1\", \"title\": \"Impact of Pretraining Word Co-occurrence on Compositional Generalization in Multimodal Models\", \"authors\": [\"Helen Qu\", \"Sang Michael Xie\"], \"summary\": \"CLIP and large multimodal models (LMMs) have better accuracy on examples\\ninvolving concepts that are highly represented in the training data. However,\\nthe role of concept combinations in the training data on compositional\\ngeneralization is largely unclear -- for instance, how does accuracy vary when\\na common object appears in an uncommon pairing with another object? In this\\npaper, we investigate how word co-occurrence statistics in the pretraining\\ndataset (a proxy for co-occurrence of visual concepts) impacts CLIP/LMM\\nperformance. To disentangle the effects of word co-occurrence frequencies from\\nsingle-word frequencies, we measure co-occurrence with pointwise mutual\\ninformation (PMI), which normalizes the joint probability of two words\\nco-occurring by the probability of co-occurring independently. Using\\nsynthetically generated images with a variety of concept pairs, we show a\\nstrong correlation between PMI in the CLIP pretraining data and zero-shot\\naccuracy in CLIP models trained on LAION-400M (r=0.97 and 14% accuracy gap\\nbetween images in the top and bottom 5% of PMI values), demonstrating that even\\naccuracy on common concepts is affected by the combination of concepts in the\\nimage. Leveraging this finding, we reproduce this effect in natural images by\\nediting them to contain pairs with varying PMI, resulting in a correlation of\\nr=0.75. Finally, we demonstrate that this behavior in CLIP transfers to LMMs\\nbuilt on top of CLIP (r=0.70 for TextVQA, r=0.62 for VQAv2). Our findings\\nhighlight the need for algorithms and architectures that improve compositional\\ngeneralization in multimodal models without scaling the training data\\ncombinatorially. Our code is available at\\nhttps://github.com/helenqu/multimodal-pretraining-pmi.\"}, {\"id\": \"2507.07999v1\", \"title\": \"Traceable Evidence Enhanced Visual Grounded Reasoning: Evaluation and Methodology\", \"authors\": [\"Haochen Wang\", \"Xiangtai Li\", \"Zilong Huang\", \"Anran Wang\", \"Jiacong Wang\", \"Tao Zhang\", \"Jiani Zheng\", \"Sule Bai\", \"Zijian Kang\", \"Jiashi Feng\", \"Zhuochen Wang\", \"Zhaoxiang Zhang\"], \"summary\": \"Models like OpenAI-o3 pioneer visual grounded reasoning by dynamically\\nreferencing visual regions, just like human \\\"thinking with images\\\". However, no\\nbenchmark exists to evaluate these capabilities holistically. To bridge this\\ngap, we propose TreeBench (Traceable Evidence Evaluation Benchmark), a\\ndiagnostic benchmark built on three principles: (1) focused visual perception\\nof subtle targets in complex scenes, (2) traceable evidence via bounding box\\nevaluation, and (3) second-order reasoning to test object interactions and\\nspatial hierarchies beyond simple object localization. Prioritizing images with\\ndense objects, we initially sample 1K high-quality images from SA-1B, and\\nincorporate eight LMM experts to manually annotate questions, candidate\\noptions, and answers for each image. After three stages of quality control,\\nTreeBench consists of 405 challenging visual question-answering pairs, even the\\nmost advanced models struggle with this benchmark, where none of them reach 60%\\naccuracy, e.g., OpenAI-o3 scores only 54.87. Furthermore, we introduce TreeVGR\\n(Traceable Evidence Enhanced Visual Grounded Reasoning), a training paradigm to\\nsupervise localization and reasoning jointly with reinforcement learning,\\nenabling accurate localizations and explainable reasoning pathways. Initialized\\nfrom Qwen2.5-VL-7B, it improves V* Bench (+16.8), MME-RealWorld (+12.6), and\\nTreeBench (+13.4), proving traceability is key to advancing vision-grounded\\nreasoning. The code is available at https://github.com/Haochen-Wang409/TreeVGR.\"}, {\"id\": \"2507.07998v1\", \"title\": \"PyVision: Agentic Vision with Dynamic Tooling\", \"authors\": [\"Shitian Zhao\", \"Haoquan Zhang\", \"Shaoheng Lin\", \"Ming Li\", \"Qilong Wu\", \"Kaipeng Zhang\", \"Chen Wei\"], \"summary\": \"LLMs are increasingly deployed as agents, systems capable of planning,\\nreasoning, and dynamically calling external tools. However, in visual\\nreasoning, prior approaches largely remain limited by predefined workflows and\\nstatic toolsets. In this report, we present PyVision, an interactive,\\nmulti-turn framework that enables MLLMs to autonomously generate, execute, and\\nrefine Python-based tools tailored to the task at hand, unlocking flexible and\\ninterpretable problem-solving. We develop a taxonomy of the tools created by\\nPyVision and analyze their usage across a diverse set of benchmarks.\\nQuantitatively, PyVision achieves consistent performance gains, boosting\\nGPT-4.1 by +7.8% on V* and Claude-4.0-Sonnet by +31.1% on VLMsAreBlind-mini.\\nThese results point to a broader shift: dynamic tooling allows models not just\\nto use tools, but to invent them, advancing toward more agentic visual\\nreasoning.\"}, {\"id\": \"2507.07997v1\", \"title\": \"MGVQ: Could VQ-VAE Beat VAE? A Generalizable Tokenizer with Multi-group Quantization\", \"authors\": [\"Mingkai Jia\", \"Wei Yin\", \"Xiaotao Hu\", \"Jiaxin Guo\", \"Xiaoyang Guo\", \"Qian Zhang\", \"Xiao-Xiao Long\", \"Ping Tan\"], \"summary\": \"Vector Quantized Variational Autoencoders (VQ-VAEs) are fundamental models\\nthat compress continuous visual data into discrete tokens. Existing methods\\nhave tried to improve the quantization strategy for better reconstruction\\nquality, however, there still exists a large gap between VQ-VAEs and VAEs. To\\nnarrow this gap, we propose \\\\NickName, a novel method to augment the\\nrepresentation capability of discrete codebooks, facilitating easier\\noptimization for codebooks and minimizing information loss, thereby enhancing\\nreconstruction quality. Specifically, we propose to retain the latent dimension\\nto preserve encoded features and incorporate a set of sub-codebooks for\\nquantization. Furthermore, we construct comprehensive zero-shot benchmarks\\nfeaturin...[截断]，共计16529字符，剩余10529字符"
        },
        "execution_time": 2.6036057472229004,
        "is_functional_test": true
      },
      {
        "case_name": "Long Query String",
        "purpose": "验证长查询字符串的处理能力",
        "args": {
          "query": "A very long search query that exceeds normal length expectations just to test how the system handles it",
          "max_results": 2
        },
        "response": {
          "result": "[{\"id\": \"2507.08000v1\", \"title\": \"Impact of Pretraining Word Co-occurrence on Compositional Generalization in Multimodal Models\", \"authors\": [\"Helen Qu\", \"Sang Michael Xie\"], \"summary\": \"CLIP and large multimodal models (LMMs) have better accuracy on examples\\ninvolving concepts that are highly represented in the training data. However,\\nthe role of concept combinations in the training data on compositional\\ngeneralization is largely unclear -- for instance, how does accuracy vary when\\na common object appears in an uncommon pairing with another object? In this\\npaper, we investigate how word co-occurrence statistics in the pretraining\\ndataset (a proxy for co-occurrence of visual concepts) impacts CLIP/LMM\\nperformance. To disentangle the effects of word co-occurrence frequencies from\\nsingle-word frequencies, we measure co-occurrence with pointwise mutual\\ninformation (PMI), which normalizes the joint probability of two words\\nco-occurring by the probability of co-occurring independently. Using\\nsynthetically generated images with a variety of concept pairs, we show a\\nstrong correlation between PMI in the CLIP pretraining data and zero-shot\\naccuracy in CLIP models trained on LAION-400M (r=0.97 and 14% accuracy gap\\nbetween images in the top and bottom 5% of PMI values), demonstrating that even\\naccuracy on common concepts is affected by the combination of concepts in the\\nimage. Leveraging this finding, we reproduce this effect in natural images by\\nediting them to contain pairs with varying PMI, resulting in a correlation of\\nr=0.75. Finally, we demonstrate that this behavior in CLIP transfers to LMMs\\nbuilt on top of CLIP (r=0.70 for TextVQA, r=0.62 for VQAv2). Our findings\\nhighlight the need for algorithms and architectures that improve compositional\\ngeneralization in multimodal models without scaling the training data\\ncombinatorially. Our code is available at\\nhttps://github.com/helenqu/multimodal-pretraining-pmi.\"}, {\"id\": \"2507.07999v1\", \"title\": \"Traceable Evidence Enhanced Visual Grounded Reasoning: Evaluation and Methodology\", \"authors\": [\"Haochen Wang\", \"Xiangtai Li\", \"Zilong Huang\", \"Anran Wang\", \"Jiacong Wang\", \"Tao Zhang\", \"Jiani Zheng\", \"Sule Bai\", \"Zijian Kang\", \"Jiashi Feng\", \"Zhuochen Wang\", \"Zhaoxiang Zhang\"], \"summary\": \"Models like OpenAI-o3 pioneer visual grounded reasoning by dynamically\\nreferencing visual regions, just like human \\\"thinking with images\\\". However, no\\nbenchmark exists to evaluate these capabilities holistically. To bridge this\\ngap, we propose TreeBench (Traceable Evidence Evaluation Benchmark), a\\ndiagnostic benchmark built on three principles: (1) focused visual perception\\nof subtle targets in complex scenes, (2) traceable evidence via bounding box\\nevaluation, and (3) second-order reasoning to test object interactions and\\nspatial hierarchies beyond simple object localization. Prioritizing images with\\ndense objects, we initially sample 1K high-quality images from SA-1B, and\\nincorporate eight LMM experts to manually annotate questions, candidate\\noptions, and answers for each image. After three stages of quality control,\\nTreeBench consists of 405 challenging visual question-answering pairs, even the\\nmost advanced models struggle with this benchmark, where none of them reach 60%\\naccuracy, e.g., OpenAI-o3 scores only 54.87. Furthermore, we introduce TreeVGR\\n(Traceable Evidence Enhanced Visual Grounded Reasoning), a training paradigm to\\nsupervise localization and reasoning jointly with reinforcement learning,\\nenabling accurate localizations and explainable reasoning pathways. Initialized\\nfrom Qwen2.5-VL-7B, it improves V* Bench (+16.8), MME-RealWorld (+12.6), and\\nTreeBench (+13.4), proving traceability is key to advancing vision-grounded\\nreasoning. The code is available at https://github.com/Haochen-Wang409/TreeVGR.\"}]"
        },
        "execution_time": 1.875889778137207,
        "is_functional_test": true
      },
      {
        "case_name": "SQL Injection Attempt Handling",
        "purpose": "验证系统对潜在SQL注入攻击的防御能力",
        "args": {
          "query": "'; DROP TABLE papers;--"
        },
        "response": {
          "result": "[{\"id\": \"2507.07991v1\", \"title\": \"Baryonification II: Constraining feedback with X-ray and kinematic Sunyaev-Zel'dovich observations\", \"authors\": [\"Michael Kova\\u010d\", \"Andrina Nicola\", \"Jozef Bucko\", \"Aurel Schneider\", \"Robert Reischke\", \"Sambit K. Giri\", \"Romain Teyssier\", \"Matthieu Schaller\", \"Joop Schaye\"], \"summary\": \"Baryonic feedback alters the matter distribution on small and intermediate\\nscales, posing a challenge for precision cosmology. The new, component-wise\\nbaryonification (BFC) approach provides a self-consistent framework to model\\nfeedback effects for different observables. In this paper we use this framework\\nto fit kinematic Sunyaev-Zel'dovich (kSZ) observations from the Atacama\\nCosmology Telescope (ACT) alongside halo X-ray gas fractions from eROSITA,\\ninvestigating baryonic feedback in a cosmological context. We first show that\\nthe kSZ data from ACT is consistent with the gas fractions from eROSITA, both\\nsuggesting a feedback model that is stronger than what is assumed in most\\nhydrodynamical simulations. This finding is in contrast to older, pre-eROSITA\\ngas fraction measurements that point towards weaker feedback in tension with\\nthe kSZ results. We suspect these discrepancies to be due to selection bias in\\nthe pre-eROSITA sample, or differences in halo mass estimation between the two\\ndata sets. In a further step, we use the BFC model to predict the baryonic\\nsuppression of the matter power spectrum. Based on our combined fit to data\\nfrom ACT and eROSITA, we find a power spectrum suppression that exceeds the\\npercent-level at modes above $k=0.3-0.6 \\\\,h\\\\,\\\\mathrm{Mpc}^{-1}$, growing to 2-8\\npercent at $k=1\\\\,h\\\\,\\\\mathrm{Mpc}^{-1}$, and to 20-25 percent at\\n$k=5\\\\,h\\\\,\\\\mathrm{Mpc}^{-1}$, consistent with strong-feedback hydrodynamical\\nsimulations. Finally, we compare our best-fitting model to the observed gas\\ndensity and pressure profiles of massive galaxy clusters from the X-COP sample,\\nfinding excellent agreement. These results show that BFC provides a\\nself-consistent picture of feedback across mass- and length scales as well as\\ndifferent cosmological observables, thus making it promising for applications\\nto multiwavelength studies to jointly constrain cosmology and baryonic effects.\"}, {\"id\": \"2507.07990v1\", \"title\": \"Multi-Granular Spatio-Temporal Token Merging for Training-Free Acceleration of Video LLMs\", \"authors\": [\"Jeongseok Hyun\", \"Sukjun Hwang\", \"Su Ho Han\", \"Taeoh Kim\", \"Inwoong Lee\", \"Dongyoon Wee\", \"Joon-Young Lee\", \"Seon Joo Kim\", \"Minho Shim\"], \"summary\": \"Video large language models (LLMs) achieve strong video understanding by\\nleveraging a large number of spatio-temporal tokens, but suffer from quadratic\\ncomputational scaling with token count. To address this, we propose a\\ntraining-free spatio-temporal token merging method, named STTM. Our key insight\\nis to exploit local spatial and temporal redundancy in video data which has\\nbeen overlooked in prior work. STTM first transforms each frame into\\nmulti-granular spatial tokens using a coarse-to-fine search over a quadtree\\nstructure, then performs directed pairwise merging across the temporal\\ndimension. This decomposed merging approach outperforms existing token\\nreduction methods across six video QA benchmarks. Notably, STTM achieves a\\n2$\\\\times$ speed-up with only a 0.5% accuracy drop under a 50% token budget, and\\na 3$\\\\times$ speed-up with just a 2% drop under a 30% budget. Moreover, STTM is\\nquery-agnostic, allowing KV cache reuse across different questions for the same\\nvideo. The project page is available at https://www.jshyun.me/projects/sttm.\"}, {\"id\": \"2507.07984v1\", \"title\": \"OST-Bench: Evaluating the Capabilities of MLLMs in Online Spatio-temporal Scene Understanding\", \"authors\": [\"JingLi Lin\", \"Chenming Zhu\", \"Runsen Xu\", \"Xiaohan Mao\", \"Xihui Liu\", \"Tai Wang\", \"Jiangmiao Pang\"], \"summary\": \"Recent advances in multimodal large language models (MLLMs) have shown\\nremarkable capabilities in integrating vision and language for complex\\nreasoning. While most existing benchmarks evaluate models under offline\\nsettings with a fixed set of pre-recorded inputs, we introduce OST-Bench, a\\nbenchmark designed to evaluate Online Spatio-Temporal understanding from the\\nperspective of an agent actively exploring a scene. The Online aspect\\nemphasizes the need to process and reason over incrementally acquired\\nobservations, while the Spatio-Temporal component requires integrating current\\nvisual inputs with historical memory to support dynamic spatial reasoning.\\nOST-Bench better reflects the challenges of real-world embodied perception.\\nBuilt on an efficient data collection pipeline, OST-Bench consists of 1.4k\\nscenes and 10k question-answer pairs collected from ScanNet, Matterport3D, and\\nARKitScenes. We evaluate several leading MLLMs on OST-Bench and observe that\\nthey fall short on tasks requiring complex spatio-temporal reasoning. Under the\\nonline setting, their accuracy declines as the exploration horizon extends and\\nthe memory grows. Through further experimental analysis, we identify common\\nerror patterns across models and find that both complex clue-based spatial\\nreasoning demands and long-term memory retrieval requirements significantly\\ndrop model performance along two separate axes, highlighting the core\\nchallenges that must be addressed to improve online embodied reasoning. To\\nfoster further research and development in the field, our codes, dataset, and\\nbenchmark are available. Our project page is:\\nhttps://rbler1234.github.io/OSTBench.github.io/\"}, {\"id\": \"2507.07974v1\", \"title\": \"Defending Against Prompt Injection With a Few DefensiveTokens\", \"authors\": [\"Sizhe Chen\", \"Yizhu Wang\", \"Nicholas Carlini\", \"Chawin Sitawarin\", \"David Wagner\"], \"summary\": \"When large language model (LLM) systems interact with external data to\\nperform complex tasks, a new attack, namely prompt injection, becomes a\\nsignificant threat. By injecting instructions into the data accessed...[截断]，共计15678字符，剩余9678字符"
        },
        "execution_time": 6.99097752571106,
        "is_functional_test": false
      },
      {
        "case_name": "Cross-Site Scripting Handling",
        "purpose": "验证系统对潜在XSS攻击的防御能力",
        "args": {
          "query": "<script>alert('xss')</script>"
        },
        "response": {
          "result": "[{\"id\": \"2507.07887v1\", \"title\": \"Automating MD simulations for Proteins using Large language Models: NAMD-Agent\", \"authors\": [\"Achuth Chandrasekhar\", \"Amir Barati Farimani\"], \"summary\": \"Molecular dynamics simulations are an essential tool in understanding protein\\nstructure, dynamics, and function at the atomic level. However, preparing high\\nquality input files for MD simulations can be a time consuming and error prone\\nprocess. In this work, we introduce an automated pipeline that leverages Large\\nLanguage Models (LLMs), specifically Gemini 2.0 Flash, in conjunction with\\npython scripting and Selenium based web automation to streamline the generation\\nof MD input files. The pipeline exploits CHARMM GUI's comprehensive web-based\\ninterface for preparing simulation-ready inputs for NAMD. By integrating\\nGemini's code generation and iterative refinement capabilities, simulation\\nscripts are automatically written, executed, and revised to navigate CHARMM\\nGUI, extract appropriate parameters, and produce the required NAMD input files.\\nPost processing is performed using additional software to further refine the\\nsimulation outputs, thereby enabling a complete and largely hands free\\nworkflow. Our results demonstrate that this approach reduces setup time,\\nminimizes manual errors, and offers a scalable solution for handling multiple\\nprotein systems in parallel. This automated framework paves the way for broader\\napplication of LLMs in computational structural biology, offering a robust and\\nadaptable platform for future developments in simulation automation.\"}, {\"id\": \"2507.07776v1\", \"title\": \"SCOOTER: A Human Evaluation Framework for Unrestricted Adversarial Examples\", \"authors\": [\"Dren Fazlija\", \"Monty-Maximilian Z\\u00fchlke\", \"Johanna Schrader\", \"Arkadij Orlov\", \"Clara Stein\", \"Iyiola E. Olatunji\", \"Daniel Kudenko\"], \"summary\": \"Unrestricted adversarial attacks aim to fool computer vision models without\\nbeing constrained by $\\\\ell_p$-norm bounds to remain imperceptible to humans,\\nfor example, by changing an object's color. This allows attackers to circumvent\\ntraditional, norm-bounded defense strategies such as adversarial training or\\ncertified defense strategies. However, due to their unrestricted nature, there\\nare also no guarantees of norm-based imperceptibility, necessitating human\\nevaluations to verify just how authentic these adversarial examples look. While\\nsome related work assesses this vital quality of adversarial attacks, none\\nprovide statistically significant insights. This issue necessitates a unified\\nframework that supports and streamlines such an assessment for evaluating and\\ncomparing unrestricted attacks. To close this gap, we introduce SCOOTER - an\\nopen-source, statistically powered framework for evaluating unrestricted\\nadversarial examples. Our contributions are: $(i)$ best-practice guidelines for\\ncrowd-study power, compensation, and Likert equivalence bounds to measure\\nimperceptibility; $(ii)$ the first large-scale human vs. model comparison\\nacross 346 human participants showing that three color-space attacks and three\\ndiffusion-based attacks fail to produce imperceptible images. Furthermore, we\\nfound that GPT-4o can serve as a preliminary test for imperceptibility, but it\\nonly consistently detects adversarial examples for four out of six tested\\nattacks; $(iii)$ open-source software tools, including a browser-based task\\ntemplate to collect annotations and analysis scripts in Python and R; $(iv)$ an\\nImageNet-derived benchmark dataset containing 3K real images, 7K adversarial\\nexamples, and over 34K human ratings. Our findings demonstrate that automated\\nvision systems do not align with human perception, reinforcing the need for a\\nground-truth SCOOTER benchmark.\"}, {\"id\": \"2507.07593v1\", \"title\": \"CleanQRL: Lightweight Single-file Implementations of Quantum Reinforcement Learning Algorithms\", \"authors\": [\"Georg Kruse\", \"Rodrigo Coelho\", \"Andreas Rosskopf\", \"Robert Wille\", \"Jeanette Miriam Lorenz\"], \"summary\": \"At the interception between quantum computing and machine learning, Quantum\\nReinforcement Learning (QRL) has emerged as a promising research field. Due to\\nits novelty, a standardized and comprehensive collection for QRL algorithms has\\nnot yet been established. Researchers rely on numerous software stacks for\\nclassical Reinforcement Learning (RL) as well as on various quantum computing\\nframeworks for the implementation of the quantum subroutines of their QRL\\nalgorithms. Inspired by the CleanRL library for classical RL algorithms, we\\npresent CleanQRL, a library that offers single-script implementations of many\\nQRL algorithms. Our library provides clear and easy to understand scripts that\\nresearchers can quickly adapt to their own needs. Alongside ray tune for\\ndistributed computing and streamlined hyperparameter tuning, CleanQRL uses\\nweights&biases to log important metrics, which facilitates benchmarking against\\nother classical and quantum implementations. The CleanQRL library enables\\nresearchers to easily transition from theoretical considerations to practical\\napplications.\"}, {\"id\": \"2507.06948v1\", \"title\": \"MCCD: A Multi-Attribute Chinese Calligraphy Character Dataset Annotated with Script Styles, Dynasties, and Calligraphers\", \"authors\": [\"Yixin Zhao\", \"Yuyi Zhang\", \"Lianwen Jin\"], \"summary\": \"Research on the attribute information of calligraphy, such as styles,\\ndynasties, and calligraphers, holds significant cultural and historical value.\\nHowever, the styles of Chinese calligraphy characters have evolved dramatically\\nthrough different dynasties and the unique touches of calligraphers, making it\\nhighly challenging to accurately recognize these different characters and their\\nattributes. Furthermore, existing calligraphic datasets are extremely scarce,\\nand most provide only character-level annotations without additional attribute\\ninformation. This limitation has significantly hindered the in-depth study of\\nChinese callig...[截断]，共计16352字符，剩余10352字符"
        },
        "execution_time": 1.8517093658447266,
        "is_functional_test": false
      },
      {
        "case_name": "Zero Results Request",
        "purpose": "验证max_results为0时的边界条件处理",
        "args": {
          "query": "robotics",
          "max_results": 0
        },
        "response": {
          "result": "{\"error\": \"An error occurred during search: Page of results was unexpectedly empty (https://export.arxiv.org/api/query?search_query=robotics&id_list=&sortBy=submittedDate&sortOrder=descending&start=700&max_results=100)\"}"
        },
        "execution_time": 39.117703914642334,
        "is_functional_test": false
      }
    ],
    "download_paper": [
      {
        "case_name": "Basic Paper Download",
        "purpose": "验证使用有效arXiv ID下载论文的基本功能",
        "args": {
          "paper_id": "2301.12345"
        },
        "response": {
          "error": "Tool call 'download_paper' was cancelled."
        },
        "execution_time": 60.00564384460449,
        "is_functional_test": true
      },
      {
        "case_name": "Download Non-Existent Paper",
        "purpose": "验证尝试下载不存在的论文时的错误处理",
        "args": {
          "paper_id": "9999.99999"
        },
        "response": {
          "result": "{\"error\": \"Paper '9999.99999' not found.\"}"
        },
        "execution_time": 48.435441732406616,
        "is_functional_test": false
      },
      {
        "case_name": "Invalid Paper ID Format",
        "purpose": "验证格式不正确的arXiv ID是否被正确拒绝",
        "args": {
          "paper_id": "invalid_id_format"
        },
        "response": {
          "result": "{\"error\": \"An unexpected error occurred: Page request resulted in HTTP 400 (https://export.arxiv.org/api/query?search_query=&id_list=invalid_id_format&sortBy=relevance&sortOrder=descending&start=0&max_results=100)\"}"
        },
        "execution_time": 11.964286088943481,
        "is_functional_test": false
      },
      {
        "case_name": "Empty Paper ID",
        "purpose": "验证空arXiv ID输入时的错误处理",
        "args": {
          "paper_id": ""
        },
        "response": {
          "result": "{\"error\": \"Paper '' not found.\"}"
        },
        "execution_time": 1.5629091262817383,
        "is_functional_test": false
      },
      {
        "case_name": "Paper ID with Special Characters",
        "purpose": "验证包含特殊字符的arXiv ID是否被正确拒绝",
        "args": {
          "paper_id": "2301.12!@#"
        },
        "response": {
          "result": "{\"error\": \"An unexpected error occurred: Page request resulted in HTTP 400 (https://export.arxiv.org/api/query?search_query=&id_list=2301.12%21%40%23&sortBy=relevance&sortOrder=descending&start=0&max_results=100)\"}"
        },
        "execution_time": 12.466046333312988,
        "is_functional_test": false
      },
      {
        "case_name": "Paper ID Length Boundary Test",
        "purpose": "验证最小长度和最大允许长度的arXiv ID是否被正确处理",
        "args": {
          "paper_id": "0001.00001"
        },
        "response": {
          "result": "{\"error\": \"Paper '0001.00001' not found.\"}"
        },
        "execution_time": 1.4054079055786133,
        "is_functional_test": true
      },
      {
        "case_name": "Ensure PDF File is Saved Correctly",
        "purpose": "验证下载的PDF文件是否以正确的名称保存到本地目录",
        "args": {
          "paper_id": "2301.12345"
        },
        "response": {
          "error": "Tool call 'download_paper' was cancelled."
        },
        "execution_time": 60.018670082092285,
        "is_functional_test": true
      },
      {
        "case_name": "Download Paper Without Internet Connection",
        "purpose": "验证无网络连接时工具是否能优雅地处理错误",
        "args": {
          "paper_id": "2301.12345"
        },
        "response": {
          "error": "Tool call 'download_paper' was cancelled."
        },
        "execution_time": 60.01742219924927,
        "is_functional_test": false
      },
      {
        "case_name": "Directory Write Permission Denied",
        "purpose": "验证没有写入权限时工具是否能正确处理错误",
        "args": {
          "paper_id": "2301.12345"
        },
        "response": {
          "error": "Tool call 'download_paper' was cancelled."
        },
        "execution_time": 60.0163209438324,
        "is_functional_test": false
      },
      {
        "case_name": "Long Paper ID Input",
        "purpose": "验证超长arXiv ID输入时的边界条件处理",
        "args": {
          "paper_id": "2301.123456789012345678901234567890"
        },
        "response": {
          "error": "Tool call 'download_paper' was cancelled."
        },
        "execution_time": 60.00926852226257,
        "is_functional_test": false
      },
      {
        "case_name": "SQL Injection Attempt in Paper ID",
        "purpose": "验证系统对潜在SQL注入攻击的防御能力",
        "args": {
          "paper_id": "' OR '1'='1"
        },
        "response": {
          "error": "Tool call 'download_paper' was cancelled."
        },
        "execution_time": 59.98995566368103,
        "is_functional_test": false
      },
      {
        "case_name": "XSS Attempt in Paper ID",
        "purpose": "验证系统对潜在跨站脚本攻击的防御能力",
        "args": {
          "paper_id": "<script>alert('xss')</script>"
        },
        "response": {
          "error": "Tool call 'download_paper' was cancelled."
        },
        "execution_time": 60.01086091995239,
        "is_functional_test": false
      }
    ],
    "list_papers": [
      {
        "case_name": "Basic List Functionality",
        "purpose": "验证list_papers工具的基本功能，当papers目录中存在PDF文件时是否能正确列出",
        "args": {},
        "response": {
          "error": "Tool call 'list_papers' was cancelled."
        },
        "execution_time": 60.004257678985596,
        "is_functional_test": true
      },
      {
        "case_name": "Empty Directory Handling",
        "purpose": "验证当papers目录存在但为空时，工具是否返回空列表",
        "args": {},
        "response": {
          "error": "Tool call 'list_papers' was cancelled."
        },
        "execution_time": 60.01310658454895,
        "is_functional_test": true
      },
      {
        "case_name": "Non-Existent Directory Handling",
        "purpose": "验证当papers目录不存在时，工具是否优雅地返回空列表",
        "args": {},
        "response": {
          "error": "Tool call 'list_papers' was cancelled."
        },
        "execution_time": 60.01249980926514,
        "is_functional_test": true
      },
      {
        "case_name": "Hidden Files Exclusion",
        "purpose": "验证工具是否忽略隐藏的PDF文件（以点开头）",
        "args": {},
        "response": {
          "error": "Tool call 'list_papers' was cancelled."
        },
        "execution_time": 60.02630925178528,
        "is_functional_test": true
      },
      {
        "case_name": "Non-PDF Files Ignored",
        "purpose": "验证工具是否仅列出.pdf扩展名文件，忽略其他格式如.docx或.txt",
        "args": {},
        "response": {
          "result": "[\"2301.12345.pdf\"]"
        },
        "execution_time": 13.595948696136475,
        "is_functional_test": true
      },
      {
        "case_name": "Special Characters in Filenames",
        "purpose": "验证包含特殊字符的PDF文件名是否被正确处理和返回",
        "args": {},
        "response": {
          "result": "[\"2301.12345.pdf\"]"
        },
        "execution_time": 0.0029973983764648438,
        "is_functional_test": true
      },
      {
        "case_name": "Long Filename Handling",
        "purpose": "验证超长PDF文件名是否被正确处理并完整返回",
        "args": {},
        "response": {
          "result": "[\"2301.12345.pdf\"]"
        },
        "execution_time": 0.0030007362365722656,
        "is_functional_test": true
      },
      {
        "case_name": "Unicode Filename Support",
        "purpose": "验证中文或其他非ASCII字符组成的PDF文件名是否被正确处理",
        "args": {},
        "response": {
          "result": "[\"2301.12345.pdf\"]"
        },
        "execution_time": 0.005513906478881836,
        "is_functional_test": true
      },
      {
        "case_name": "Directory Read Permission Denied",
        "purpose": "验证没有读取权限的papers目录是否被正确处理并返回错误对象",
        "args": {},
        "response": {
          "result": "[\"2301.12345.pdf\"]"
        },
        "execution_time": 0.00450444221496582,
        "is_functional_test": false
      },
      {
        "case_name": "File System Error Handling",
        "purpose": "验证文件系统错误（如磁盘损坏）时工具是否返回带有error键的JSON对象",
        "args": {},
        "response": {
          "result": "[\"2301.12345.pdf\"]"
        },
        "execution_time": 0.004006624221801758,
        "is_functional_test": false
      },
      {
        "case_name": "Symbolic Link Handling",
        "purpose": "验证工具是否正确处理指向外部位置的符号链接而不暴露路径信息",
        "args": {},
        "response": {
          "result": "[\"2301.12345.pdf\"]"
        },
        "execution_time": 0.00651097297668457,
        "is_functional_test": false
      },
      {
        "case_name": "Malformed PDF File Handling",
        "purpose": "验证虽然文件扩展名为.pdf但实际不是有效PDF文件的情况是否仍被列出",
        "args": {},
        "response": {
          "result": "[\"2301.12345.pdf\"]"
        },
        "execution_time": 0.004513263702392578,
        "is_functional_test": true
      }
    ],
    "read_paper": [
      {
        "case_name": "Basic PDF Reading",
        "purpose": "验证从本地papers目录读取并提取PDF文件文本内容的基本功能",
        "args": {
          "filename": "2023.nlposs-1.24.pdf"
        },
        "response": {
          "result": "{\"error\": \"File '2023.nlposs-1.24.pdf' not found.\"}"
        },
        "execution_time": 0.004511117935180664,
        "is_functional_test": true
      },
      {
        "case_name": "File Not Found Error Handling",
        "purpose": "验证尝试读取不存在的PDF文件时是否返回包含error键的JSON对象",
        "args": {
          "filename": "nonexistent_file.pdf"
        },
        "response": {
          "result": "{\"error\": \"File 'nonexistent_file.pdf' not found.\"}"
        },
        "execution_time": 0.004007101058959961,
        "is_functional_test": false
      },
      {
        "case_name": "Empty Filename Input",
        "purpose": "验证空文件名输入是否被正确处理并返回错误信息",
        "args": {
          "filename": ""
        },
        "response": {
          "result": "{\"error\": \"An error occurred while reading the paper: 'papers\\\\' is no file\"}"
        },
        "execution_time": 0.0040357112884521484,
        "is_functional_test": false
      },
      {
        "case_name": "Special Characters in Filename",
        "purpose": "验证包含特殊字符的文件名是否能被安全处理",
        "args": {
          "filename": "special@#%&()_file.pdf"
        },
        "response": {
          "result": "{\"error\": \"File 'special@#%&()_file.pdf' not found.\"}"
        },
        "execution_time": 0.004513263702392578,
        "is_functional_test": true
      },
      {
        "case_name": "Reading File with Unicode Content",
        "purpose": "验证包含Unicode字符的PDF文件是否能够被正确读取和返回",
        "args": {
          "filename": "unicode_content.pdf"
        },
        "response": {
          "result": "{\"error\": \"File 'unicode_content.pdf' not found.\"}"
        },
        "execution_time": 0.007565736770629883,
        "is_functional_test": true
      },
      {
        "case_name": "Read Non-PDF File Attempt",
        "purpose": "验证尝试读取非PDF格式文件（如.txt）时是否返回错误",
        "args": {
          "filename": "test_output.txt"
        },
        "response": {
          "result": "{\"error\": \"File 'test_output.txt' not found.\"}"
        },
        "execution_time": 0.0039975643157958984,
        "is_functional_test": false
      },
      {
        "case_name": "Large PDF File Reading",
        "purpose": "验证大体积PDF文件是否能被成功读取且不导致内存溢出",
        "args": {
          "filename": "道路交通安全违法行为处理程序规定.pdf"
        },
        "response": {
          "result": "{\"error\": \"File '\\u9053\\u8def\\u4ea4\\u901a\\u5b89\\u5168\\u8fdd\\u6cd5\\u884c\\u4e3a\\u5904\\u7406\\u7a0b\\u5e8f\\u89c4\\u5b9a.pdf' not found.\"}"
        },
        "execution_time": 0.005516529083251953,
        "is_functional_test": true
      },
      {
        "case_name": "Empty PDF File Handling",
        "purpose": "验证内容为空的PDF文件是否能被正确处理而不抛出异常",
        "args": {
          "filename": "empty_content.pdf"
        },
        "response": {
          "result": "{\"error\": \"File 'empty_content.pdf' not found.\"}"
        },
        "execution_time": 0.005992889404296875,
        "is_functional_test": true
      },
      {
        "case_name": "Long Filename Handling",
        "purpose": "验证超长文件名的PDF是否能被正确读取",
        "args": {
          "filename": "very_long_filename_that_exceeds_normal_length_expectations_but_still_valid.pdf"
        },
        "response": {
          "result": "{\"error\": \"File 'very_long_filename_that_exceeds_normal_length_expectations_but_still_valid.pdf' not found.\"}"
        },
        "execution_time": 0.004001617431640625,
        "is_functional_test": true
      },
      {
        "case_name": "SQL Injection in Filename",
        "purpose": "验证系统对潜在SQL注入攻击的防御能力，使用恶意构造的文件名",
        "args": {
          "filename": "'; DROP TABLE papers;--.pdf"
        },
        "response": {
          "result": "{\"error\": \"File ''; DROP TABLE papers;--.pdf' not found.\"}"
        },
        "execution_time": 0.0039997100830078125,
        "is_functional_test": false
      },
      {
        "case_name": "XSS Attempt in Filename",
        "purpose": "验证系统对潜在跨站脚本攻击的防御能力，使用包含脚本的文件名",
        "args": {
          "filename": "<script>alert('xss')</script>.pdf"
        },
        "response": {
          "result": "{\"error\": \"File '<script>alert('xss')</script>.pdf' not found.\"}"
        },
        "execution_time": 0.004518747329711914,
        "is_functional_test": false
      },
      {
        "case_name": "Directory Traversal Attempt",
        "purpose": "验证系统对路径穿越攻击的防护能力，防止访问受限目录中的文件",
        "args": {
          "filename": "../../etc/passwd.pdf"
        },
        "response": {
          "result": "{\"error\": \"File '../../etc/passwd.pdf' not found.\"}"
        },
        "execution_time": 0.010570526123046875,
        "is_functional_test": false
      }
    ]
  },
  "total_cases": 48
}