import glob
import os
import sys
import argparse
import json
import asyncio
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional
import numpy as np
import matplotlib.pyplot as plt
import concurrent.futures

# Ensure other modules can be imported
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import testing modules
from testSystem.log_analyzer import LogAnalyzer
from testSystem.intelligent_benchmark import MCPIntelligentTester
from testSystem.reporting import SCORE_WEIGHTS, SCORE_DIMENSIONS
from testSystem.utils.utils import load_server_mapping, PIPELINE_MODELS
from testSystem.utils.report_generator import (
    generate_pipeline_comparison_report,
    generate_comparison_report,
    generate_benchmark_summary,
)

def parse_args():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(description="MCP Multi-Agent System Testing Tool")
    
    parser.add_argument(
        "--mode", 
        choices=["log", "public", "refinement", "compare", "analyze-benchmark", "pipeline", "metagpt"],
        default="public",
        help="Run mode: log(log analysis), public(test public servers), refinement(test MCP-GEN generated servers), compare(horizontal comparison test), analyze-benchmark(independent analysis of Benchmark logs), pipeline(test new model servers), metagpt(test MetaGPT generated servers)"
    )
    
    parser.add_argument(
        "--concurrency",
        type=int,
        default=1,
        help="Number of concurrent tests (default is 1, i.e., serial)"
    )
    
    parser.add_argument(
        "--log-dir",
        default="logs/agent_logs/",
        help="Agent log directory path"
    )
    
    parser.add_argument(
        "--public-servers-dir",
        default="workspace/public-mcp-servers/",
        help="Public MCP servers directory path"
    )
    
    parser.add_argument(
        "--refinement-servers-dir",
        default="workspace/refinement/",
        help="MCP-GEN generated MCP servers directory path"
    )

    parser.add_argument(
        "--metagpt-servers-dir",
        default="workspace/metaGPT-servers/",
        help="MetaGPT generated MCP servers directory path"
    )

    parser.add_argument(
        "--mapping-file",
        default="workspace/reproduce_mapping.txt",
        help="Name mapping file between public servers and MCP-GEN generated servers"
    )
    
    parser.add_argument(
        "--pipeline-mapping-file",
        default="workspace/pipeline_mapping.json",
        help="Mapping file for servers generated by Pipeline models"
    )
    
    parser.add_argument(
        "--output-dir",
        default="reports",
        help="Report output directory"
    )
    
    return parser.parse_args()

def run_log_analysis(log_dir, output_dir, log_files=None):
    """Run log analysis
    
    Args:
        log_dir: Log directory path
        output_dir: Report output directory
        log_files: (Optional) Specify list of log files to analyze
    """
    print(f"\n====== Starting Log Analysis ({output_dir}) ======")
    
    # Create analyzer
    analyzer = LogAnalyzer(log_dir=log_dir)
    
    # Load logs
    print("Loading log files...")
    # If log_files specified, only load these files
    agents_data = analyzer.load_logs(log_files=log_files)
    if not agents_data:
        print("No log data loaded, skipping analysis.")
        return None
    print(f"Loaded logs for {len(agents_data)} agents")
    
    # Save log analysis report
    log_output_dir = os.path.join(output_dir) # Use the passed output_dir directly
    os.makedirs(log_output_dir, exist_ok=True)
    
    # Define unified timestamp
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # Check if it's Benchmark log
    is_benchmark_log = False
    if log_files and any("BenchmarkTester-Agent" in os.path.basename(f) for f in log_files):
        is_benchmark_log = True

    # Create different reports based on log type
    if is_benchmark_log:
        print("Benchmark log detected, generating dedicated report...")
        report = analyzer.create_benchmark_report()
        report_path = os.path.join(log_output_dir, f"benchmark_analysis_{timestamp}.json")
        
        # Visualization
        print("Generating Benchmark visualization charts...")
        viz_dir = os.path.join(log_output_dir, "benchmark_visualizations")
        analyzer.visualize_benchmark_report(report, output_dir=viz_dir)
        print(f"Visualization charts saved to {viz_dir}")

    else:
        print("Generating academic performance report...")
        report = analyzer.create_academic_report()
        report_path = os.path.join(log_output_dir, f"log_analysis_{timestamp}.json")
        
        # Generate visualizations
        print("Generating visualization charts...")
        viz_dir = os.path.join(log_output_dir, "visualizations")
        analyzer.visualize_academic_metrics(output_dir=viz_dir)
        print(f"Visualization charts saved to {viz_dir}")

    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"Analysis report saved to {report_path}")
    
    return report

def run_batch_tests(servers_dir: str, output_dir_name: str, output_dir: str, log_dir: str, concurrency: int) -> Dict:
    """Concurrently test MCP servers in specified directory
    
    Args:
        servers_dir: MCP servers directory path
        output_dir_name: Report subdirectory name
        output_dir: Report output directory
        log_dir: Agent log directory path
        concurrency: Number of concurrent tests
        
    Returns:
        Test results dictionary
    """
    print(f"\n====== Starting Batch Testing {servers_dir} (Concurrency: {concurrency}) ======")
  
    # Find all server.py files in project directories
    server_files = []
    target_dir = Path(servers_dir)
    
    if not target_dir.exists():
        print(f"Error: Directory {servers_dir} does not exist")
        return {}
        
    # Check if word_mcp_server_document_opera_refined directory exists
    priority_server = None
    for project_dir in [d for d in target_dir.iterdir() if d.is_dir()]:
        if project_dir.name == "word_mcp_server_document_opera_refined":
            server_file = project_dir / "server.py"
            if server_file.exists():
                priority_server = str(server_file)
                print(f"Detected priority test target: {project_dir.name}")
                break
        
    for project_dir in sorted([d for d in target_dir.iterdir() if d.is_dir()]):
        server_file = project_dir / "server.py"
        if server_file.exists() and str(server_file) != priority_server:
            server_files.append(str(server_file))
            
    # If priority server exists, put it at the front of the list
    if priority_server:
        server_files.insert(0, priority_server)
            
    if not server_files:
        print(f"No server.py files found in {servers_dir} directory")
        return {}
    
    print(f"Found {len(server_files)} server files to test: {[Path(sf).parent.name for sf in server_files]}")
    
    final_benchmark_results = {}

    # Use ThreadPoolExecutor for true parallel testing
    with concurrent.futures.ThreadPoolExecutor(max_workers=concurrency) as executor:
        # Submit all test tasks
        future_to_server = {executor.submit(run_single_test_worker, sf, Path(sf).parent.name, output_dir, output_dir_name): sf for sf in server_files}
        
        for future in concurrent.futures.as_completed(future_to_server):
            server_file = future_to_server[future]
            project_name = Path(server_file).parent.name
            try:
                # Get result of single test
                result = future.result()
                if result:
                    final_benchmark_results.update(result)
            except Exception as exc:
                print(f"[{datetime.now().strftime('%H:%M:%S')}] ❌ Test generated critical error: {project_name} - {exc}")
                import traceback
                traceback.print_exc()

    # After all tests complete, uniformly perform log analysis
    all_benchmark_reports = analyze_all_logs(final_benchmark_results, log_dir, output_dir, output_dir_name)
    
    # Generate overall Benchmark analysis report
    if all_benchmark_reports:
        generate_benchmark_summary(all_benchmark_reports, os.path.join(output_dir, output_dir_name))

    return final_benchmark_results

def run_single_test_worker(server_file: str, project_name: str, output_dir: str, output_dir_name: str, model_name: Optional[str] = None) -> Dict:
    """
    Single test task worker unit, executed in independent thread.
    """
    import threading
    import signal
    import time
    from functools import partial

    report_base_dir = os.path.join(output_dir, output_dir_name)
    if model_name:
        report_base_dir = os.path.join(report_base_dir, model_name)
    os.makedirs(report_base_dir, exist_ok=True) # Ensure directory exists

    # Set timeout (30 minutes)
    task_timeout = 30 * 60  # seconds
    results = {}
    task_completed = threading.Event()
    task_terminated = threading.Event()
    
    # Define test task
    def test_task():
        nonlocal results
        try:
            tester = MCPIntelligentTester(
                output_dir=report_base_dir, 
                project_name_override=project_name,
                # Set stricter tool timeout and consecutive timeout count
                tool_timeout=50.0,  # 50 second tool timeout
                max_consecutive_timeouts=2  # Skip after 2 consecutive timeouts
            )
            
            print(f"[{datetime.now().strftime('%H:%M:%S')}] >> Starting test: {project_name}")
            
            # Execute test
            task_results = tester.execute_test_suite(server_file)
            
            if task_results and "total_score" in task_results:
                score = task_results.get("total_score", "N/A")
                print(f"[{datetime.now().strftime('%H:%M:%S')}] ✅ Test completed: {project_name} | Score: {score}")
            else:
                error_msg = task_results.get("error", "Unknown error") if task_results else "No result returned"
                print(f"[{datetime.now().strftime('%H:%M:%S')}] ⚠️ Test completed but no valid score: {project_name} - {error_msg}")
            
            # Save results
            results = tester.benchmark_results
            
        except Exception as e:
            print(f"[{datetime.now().strftime('%H:%M:%S')}] ❌ Test error: {project_name} - {e}")
            import traceback
            traceback.print_exc()
        finally:
            # Mark task completion
            if not task_terminated.is_set():
                task_completed.set()
    
    # Define timeout handler function
    def timeout_handler():
        if not task_completed.is_set():
            print(f"[{datetime.now().strftime('%H:%M:%S')}] ⚠️ Test task timeout ({task_timeout} seconds): {project_name}")
            task_terminated.set()
            
            # Create basic timeout result
            timeout_result_name = f"{project_name}"
            results = {
                timeout_result_name: {
                    "server_name": os.path.basename(server_file).replace('.py', ''),
                    "parent_dir": os.path.basename(os.path.dirname(os.path.abspath(server_file))),
                    "report_name": timeout_result_name,
                    "server_path": server_file,
                    "timestamp": datetime.now().isoformat(),
                    "error": "Test task timeout (30 minutes)",
                    "abnormal_termination": "Task timeout after 30 minutes",
                    "tools": [],
                    "test_results": {},
                    "total_cases": 0,
                    "total_score": 0,
                    "scores": {dim: 0 for dim in SCORE_DIMENSIONS}
                }
            }
            
            # Create simple timeout report file
            try:
                timeout_report_path = os.path.join(report_base_dir, f"test_report_{timeout_result_name}.json")
                with open(timeout_report_path, 'w', encoding='utf-8') as f:
                    json.dump(results, f, ensure_ascii=False, indent=2)
                print(f"[{datetime.now().strftime('%H:%M:%S')}] Timeout report created: {timeout_report_path}")
            except Exception as e:
                print(f"Failed to create timeout report: {e}")

    # Start test thread
    test_thread = threading.Thread(target=test_task)
    test_thread.daemon = True
    test_thread.start()
    
    # Start timeout monitoring thread
    timer_thread = threading.Thread(target=lambda: (time.sleep(task_timeout), timeout_handler()))
    timer_thread.daemon = True
    timer_thread.start()
    
    # Wait for test completion or timeout
    while not (task_completed.is_set() or task_terminated.is_set()):
        time.sleep(1)  # Check status every second
        
    # If terminated due to timeout, wait for timeout handling to complete
    if task_terminated.is_set():
        time.sleep(2)

    return results

def analyze_all_logs(benchmark_results: Dict, log_dir: str, output_dir: str, output_dir_name: str, report_to_model_map: Optional[Dict[str, str]] = None) -> List[Dict]:
    """
    Analyze all log files generated by test runs.
    """
    if not benchmark_results:
        return []
        
    print("\n====== Starting Analysis of Test Logs ======")
    all_benchmark_reports = []
    
    # Special handling for pipeline or metagpt mode, group by model
    if report_to_model_map:
        logs_by_model = {}
        for report_name in benchmark_results:
            model_name = report_to_model_map.get(report_name)
            if not model_name:
                print(f"Warning: Model name not found in mapping for {report_name}, skipping.")
                continue
            
            agent_name_prefix = f"BenchmarkTester-Agent-{report_name}"
            log_file_pattern = os.path.join(log_dir, f"{agent_name_prefix}*.jsonl")
            found_logs = glob.glob(log_file_pattern)
            
            if not found_logs:
                print(f"Warning: No log files found for {report_name}, skipping analysis.")
                continue
            
            latest_log_file = max(found_logs, key=os.path.getctime)
            
            if model_name not in logs_by_model:
                logs_by_model[model_name] = []
            logs_by_model[model_name].append(os.path.basename(latest_log_file))
            
        # Analyze by model
        for model_name, log_files in logs_by_model.items():
            print(f"\n--- Analyzing {len(log_files)} log files for model '{model_name}' ---")
            model_output_dir = os.path.join(output_dir, output_dir_name, model_name)
            os.makedirs(model_output_dir, exist_ok=True)
            
            benchmark_report = run_log_analysis(log_dir, model_output_dir, log_files=log_files)
            if benchmark_report:
                all_benchmark_reports.append(benchmark_report)
    else:
        # Use original logic for other modes
        for report_name, results in benchmark_results.items():
            agent_name_prefix = f"BenchmarkTester-Agent-{report_name}"
            
            # Find corresponding latest log file
            log_file_pattern = os.path.join(log_dir, f"{agent_name_prefix}*.jsonl")
            found_logs = glob.glob(log_file_pattern)
            
            if not found_logs:
                print(f"Warning: No log files found for {report_name}, skipping analysis.")
                continue
            
            latest_log_file = max(found_logs, key=os.path.getctime)
            print(f"\n--- Analyzing log: {os.path.basename(latest_log_file)} ---")
            
            # Create independent analysis report directory for this log
            log_analysis_output_dir = os.path.join(output_dir, output_dir_name, report_name)
            
            # Fix path issue: pass specific directory and basic name of log file
            log_file_dir = os.path.dirname(latest_log_file)
            log_file_basename = os.path.basename(latest_log_file)
            
            benchmark_report = run_log_analysis(log_file_dir, log_analysis_output_dir, log_files=[log_file_basename])
            if benchmark_report:
                all_benchmark_reports.append(benchmark_report)
    return all_benchmark_reports

def run_pipeline_tests(pipeline_mapping_file: str, output_dir: str, log_dir: str, concurrency: int) -> Dict:
    """Concurrently test servers generated by AI models defined in pipeline_mapping.json"""
    print(f"\n====== Starting Pipeline Mode Testing (Concurrency: {concurrency}) ======")
    output_dir_name = "pipeline_server_tests"
    
    # 1. Load mapping file
    try:
        with open(pipeline_mapping_file, 'r', encoding='utf-8') as f:
            mapping_data = json.load(f)
    except FileNotFoundError:
        print(f"Error: Pipeline mapping file not found: {pipeline_mapping_file}")
        return {}
    except json.JSONDecodeError:
        print(f"Error: Pipeline mapping file format error: {pipeline_mapping_file}")
        return {}

    # 2. Collect and sort tasks by model order to avoid concurrent requests to same service
    # models_to_run_this_time is only for this run, while PIPELINE_MODELS is for generating complete reports containing all target models
    models_to_run_this_time = ["deepseek-v3"]
    tasks_to_run = []

    for model_name in models_to_run_this_time:
        print(f"\n--- Preparing test tasks for model '{model_name}' ---")
        priority_tasks = []
        normal_tasks = []
        last_tasks = []

        for server_info in mapping_data:
            generated_server = server_info["generated_servers"].get(model_name)
            if generated_server and generated_server.get("server_path"):
                server_path = generated_server["server_path"]
                if os.path.exists(server_path):
                    project_name_raw = generated_server['project_name']
                    # Build unique project name, format: <model_name>-<project_name>
                    project_name = f"{model_name}-{project_name_raw}"
                    task = {"path": server_path, "name": project_name, "model": model_name}
                    
                    # Classify by priority based on project name
                    if "word_document_automation" in project_name_raw:
                        priority_tasks.append(task)
                    elif "git" in project_name_raw:
                        last_tasks.append(task)
                    else:
                        normal_tasks.append(task)
                else:
                    print(f"Warning: Server file does not exist, skipping: {server_path}")
        
        # Merge current model's tasks by priority, then add to total task list
        model_tasks = priority_tasks + normal_tasks + last_tasks
        tasks_to_run.extend(model_tasks)

    if not tasks_to_run:
        print("No testable servers found in Pipeline mapping file.")
        return {}
        
    print(f"Found {len(tasks_to_run)} server files to test (sorted):")
    for task in tasks_to_run:
        print(f"  - {task['name']}")

    # 3. Execute concurrent testing
    final_benchmark_results = {}
    report_to_model_map = {}
    with concurrent.futures.ThreadPoolExecutor(max_workers=concurrency) as executor:
        future_to_task = {
            executor.submit(run_single_test_worker, task['path'], task['name'], output_dir, output_dir_name, task.get('model')): task
            for task in tasks_to_run
        }

        for future in concurrent.futures.as_completed(future_to_task):
            task = future_to_task[future]
            project_name = task['name']
            report_to_model_map[project_name] = task.get('model')
            try:
                result = future.result()
                if result:
                    final_benchmark_results.update(result)
            except Exception as exc:
                print(f"[{datetime.now().strftime('%H:%M:%S')}] ❌ Test generated critical error: {project_name} - {exc}")
                import traceback
                traceback.print_exc()

    # 4. Analyze logs and generate reports
    all_benchmark_reports = analyze_all_logs(final_benchmark_results, log_dir, output_dir, output_dir_name, report_to_model_map)
    
    if all_benchmark_reports:
        generate_benchmark_summary(all_benchmark_reports, os.path.join(output_dir, output_dir_name))

    # 5. Generate horizontal comparison report
    # By merging test results and log analysis results, create a rich dataset containing all dimensional data
    analysis_results = {}
    if all_benchmark_reports:
        for report in all_benchmark_reports:
            if not report or "benchmark_results" not in report:
                continue
            
            benchmark_dict = report.get("benchmark_results", {})
            if not benchmark_dict:
                continue

            # Key in log analysis report is unique test report name
            report_name = list(benchmark_dict.keys())[0]
            report_content = benchmark_dict[report_name]
            analysis_results[report_name] = report_content

    # Merge scores from initial test results into detailed analysis results
    for report_name, test_result in final_benchmark_results.items():
        if report_name in analysis_results:
            analysis_results[report_name]['total_score'] = test_result.get('total_score', 0)
            analysis_results[report_name]['scores'] = test_result.get('scores', {})
            # Ensure suite_summary is updated, as results directly from tests are more reliable
            if 'suite_summary' in test_result:
                analysis_results[report_name]['suite_summary'] = test_result['suite_summary']
        else:
            # If log analysis failed, still include basic score and summary information
            analysis_results[report_name] = {
                'total_score': test_result.get('total_score', 0),
                'scores': test_result.get('scores', {}),
                'suite_summary': test_result.get('suite_summary', {}) # Get from test results or use empty dict
            }

    if analysis_results:
        generate_pipeline_comparison_report(
            pipeline_results=analysis_results,  # Use merged rich data
            output_dir=os.path.join(output_dir, output_dir_name),
            mapping_file=pipeline_mapping_file,
            models_to_compare=PIPELINE_MODELS  # Pass complete model list for reporting
        )

    return final_benchmark_results

def run_metagpt_tests(metagpt_servers_dir: str, output_dir: str, log_dir: str, concurrency: int):
    """Concurrently test servers in metaGPT-servers directory"""
    print(f"\n====== Starting MetaGPT Mode Testing (Concurrency: {concurrency}) ======")
    
    # 1. Scan directory to find models and their servers
    target_dir = Path(metagpt_servers_dir)
    if not target_dir.exists():
        print(f"Error: MetaGPT servers directory not found: {metagpt_servers_dir}")
        return {}

    output_dir_name = target_dir.name # e.g., "metaGPT-servers"
    tasks_to_run = []
    # Subdirectories are model names, e.g., 'metaGPT-qwen-plus'
    for model_dir in sorted([d for d in target_dir.iterdir() if d.is_dir()]):
        model_name = model_dir.name
        print(f"\n--- Preparing test tasks for model '{model_name}' ---")
        
        # Find all python files and main.py in subdirectories
        server_files = list(model_dir.glob("*.py"))
        server_files.extend(list(model_dir.glob("*/main.py")))

        for server_path in server_files:
            if os.path.exists(server_path):
                # Derive project name from file/directory name
                if server_path.name == 'main.py':
                    project_name_raw = server_path.parent.name
                else:
                    project_name_raw = server_path.stem
                
                # Create unique project name
                project_name = f"{model_name}-{project_name_raw}"
                task = {"path": str(server_path), "name": project_name, "model": model_name}
                tasks_to_run.append(task)
            else:
                print(f"Warning: Server file does not exist, skipping: {server_path}")

    if not tasks_to_run:
        print("No testable servers found in MetaGPT servers directory.")
        return {}
        
    print(f"Found {len(tasks_to_run)} server files to test:")
    for task in tasks_to_run:
        print(f"  - {task['name']}")

    # 2. Execute concurrent testing
    final_benchmark_results = {}
    report_to_model_map = {}
    with concurrent.futures.ThreadPoolExecutor(max_workers=concurrency) as executor:
        future_to_task = {
            executor.submit(run_single_test_worker, task['path'], task['name'], output_dir, output_dir_name, task.get('model')): task
            for task in tasks_to_run
        }

        for future in concurrent.futures.as_completed(future_to_task):
            task = future_to_task[future]
            project_name = task['name']
            report_to_model_map[project_name] = task.get('model')
            try:
                result = future.result()
                if result:
                    final_benchmark_results.update(result)
            except Exception as exc:
                print(f"[{datetime.now().strftime('%H:%M:%S')}] ❌ Test generated critical error: {project_name} - {exc}")
                import traceback
                traceback.print_exc()

    # 3. Analyze logs and generate reports
    all_benchmark_reports = analyze_all_logs(final_benchmark_results, log_dir, output_dir, output_dir_name, report_to_model_map)
    
    if all_benchmark_reports:
        generate_benchmark_summary(all_benchmark_reports, os.path.join(output_dir, output_dir_name))

    return final_benchmark_results

async def main():
    """Main function"""
    args = parse_args()
    
    # Create dedicated report directory for this run
    run_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_output_dir = os.path.join(args.output_dir, f"run_{run_timestamp}")
    os.makedirs(run_output_dir, exist_ok=True)
    print(f"Reports for this run will be saved to: {os.path.abspath(run_output_dir)}")
    
    if args.mode == "log":
        run_log_analysis(args.log_dir, run_output_dir)
        
    elif args.mode == "public":
        run_batch_tests(
            servers_dir=args.public_servers_dir,
            output_dir_name="public_server_tests",
            output_dir=run_output_dir,
            log_dir=args.log_dir,
            concurrency=args.concurrency
        )

    elif args.mode == "refinement":
        run_batch_tests(
            servers_dir=args.refinement_servers_dir,
            output_dir_name="refinement_server_tests",
            output_dir=run_output_dir,
            log_dir=args.log_dir,
            concurrency=args.concurrency
        )

    elif args.mode == "compare":
        # 1. Test public servers
        public_results = run_batch_tests(
            servers_dir=args.public_servers_dir,
            output_dir_name="public_server_tests",
            output_dir=run_output_dir,
            log_dir=args.log_dir,
            concurrency=args.concurrency
        )
        
        # 2. Test optimized servers
        refinement_results = run_batch_tests(
            servers_dir=args.refinement_servers_dir,
            output_dir_name="refinement_server_tests",
            output_dir=run_output_dir,
            log_dir=args.log_dir,
            concurrency=args.concurrency
        )

        # 3. Load mapping and generate comparison report
        if public_results and refinement_results:
            mapping = load_server_mapping(args.mapping_file)
            if mapping:
                generate_comparison_report(public_results, refinement_results, mapping, run_output_dir)
            else:
                print("Failed to load server mapping relationships, cannot generate comparison report.")
        else:
            print("Failed to successfully complete testing of both server groups, cannot perform comparison.")

    elif args.mode == "analyze-benchmark":
        all_logs = glob.glob(os.path.join(args.log_dir, "BenchmarkTester-Agent-*.jsonl"))
        if not all_logs:
            print(f"No Benchmark log files found in {args.log_dir}")
            return

        latest_files = sorted(all_logs, key=os.path.getctime, reverse=True)
        # Assume maximum 100 concurrent per run
        latest_run_logs = latest_files[:100]
        
        print(f"Will analyze the latest {len(latest_run_logs)} Benchmark log files...")
        
        run_log_analysis(args.log_dir, run_output_dir, log_files=[os.path.basename(f) for f in latest_run_logs])

    elif args.mode == "pipeline":
        run_pipeline_tests(
            pipeline_mapping_file=args.pipeline_mapping_file,
            output_dir=run_output_dir,
            log_dir=args.log_dir,
            concurrency=args.concurrency
        )
    
    elif args.mode == "metagpt":
        run_metagpt_tests(
            metagpt_servers_dir=args.metagpt_servers_dir,
            output_dir=run_output_dir,
            log_dir=args.log_dir,
            concurrency=args.concurrency
        )

def main_sync():
    """Synchronous execution Main function entry"""
    args = parse_args()
    
    # Create dedicated report directory for this run
    run_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_output_dir = os.path.join(args.output_dir, f"run_{run_timestamp}")
    os.makedirs(run_output_dir, exist_ok=True)
    print(f"Reports for this run will be saved to: {os.path.abspath(run_output_dir)}")
    
    if args.mode == "log":
        run_log_analysis(args.log_dir, run_output_dir)
        
    elif args.mode == "public":
        run_batch_tests(
            servers_dir=args.public_servers_dir, 
            output_dir_name="public_server_tests",
            output_dir=run_output_dir, 
            log_dir=args.log_dir,
            concurrency=args.concurrency
        )
        
    elif args.mode == "refinement":
        run_batch_tests(
            servers_dir=args.refinement_servers_dir, 
            output_dir_name="refinement_server_tests",
            output_dir=run_output_dir,
            log_dir=args.log_dir,
            concurrency=args.concurrency
        )
        
    elif args.mode == "compare":
        # 1. Test public servers
        public_results = run_batch_tests(
            servers_dir=args.public_servers_dir,
            output_dir_name="public_server_tests",
            output_dir=run_output_dir,
            log_dir=args.log_dir,
            concurrency=args.concurrency
        )
        
        # 2. Test optimized servers
        refinement_results = run_batch_tests(
            servers_dir=args.refinement_servers_dir,
            output_dir_name="refinement_server_tests",
            output_dir=run_output_dir,
            log_dir=args.log_dir,
            concurrency=args.concurrency
        )

        # 3. Load mapping and generate comparison report
        if public_results and refinement_results:
            mapping = load_server_mapping(args.mapping_file)
            if mapping:
                generate_comparison_report(public_results, refinement_results, mapping, run_output_dir)
            else:
                print("Failed to load server mapping relationships, cannot generate comparison report.")
        else:
            print("Failed to successfully complete testing of both server groups, cannot perform comparison.")
            
    elif args.mode == "analyze-benchmark":
        all_logs = glob.glob(os.path.join(args.log_dir, "BenchmarkTester-Agent-*.jsonl"))
        if not all_logs:
            print(f"No Benchmark log files found in {args.log_dir}")
            return
            
        latest_files = sorted(all_logs, key=os.path.getctime, reverse=True)
        # Assume maximum 100 concurrent per run
        latest_run_logs = latest_files[:100]
        
        print(f"Will analyze the latest {len(latest_run_logs)} Benchmark log files...")
        
        run_log_analysis(args.log_dir, run_output_dir, log_files=[os.path.basename(f) for f in latest_run_logs])

    elif args.mode == "pipeline":
        run_pipeline_tests(
            pipeline_mapping_file=args.pipeline_mapping_file,
            output_dir=run_output_dir,
            log_dir=args.log_dir,
            concurrency=args.concurrency
        )

    elif args.mode == "metagpt":
        run_metagpt_tests(
            metagpt_servers_dir=args.metagpt_servers_dir,
            output_dir=run_output_dir,
            log_dir=args.log_dir,
            concurrency=args.concurrency
        )

if __name__ == "__main__":
    # Since our core testing is blocking, we don't use asyncio.run
    main_sync() 