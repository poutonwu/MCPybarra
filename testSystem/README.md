# MCPybarra Testing System

This is the automated testing system component of **MCPybarra**, a multi-agent framework for low-cost, high-quality MCP service generation. The testing system employs Large Language Model (LLM) driven intelligent testing methods, capable of automatically generating test cases, executing tests, and generating detailed analysis reports for MCP servers.

## Project Context

MCPybarra is a novel multi-agent framework that automates the generation of high-quality MCP services from natural language requirements. Our approach employs three specialized agents—Code Generator, Quality Assurance Inspector, and Code Refiner—operating within a stateful workflow that implements continuous quality-driven iterative refinement.

This testing system serves as the **QA Inspector Agent** component, providing comprehensive quality assessment through systematic test generation, execution, and reporting.

## Project Structure

```
testSystem/
├── main.py                 # Main program entry, supports multiple testing modes
├── intelligent_benchmark.py # MCP server intelligent testing core module
├── log_analyzer.py         # Log analysis tool
├── reporting.py            # Report generation module
├── metrics.md             # Evaluation metrics definition document
├── prompts/               # Prompt templates
│   ├── benchmark/         # Benchmark testing related prompts
│   ├── reporting/         # Report generation related prompts
│   └── utils.py          # Prompt utility functions
├── testFiles/            # Test files directory
└── server-test-report/   # Test report output directory
```

## Five-Dimensional Quality Evaluation

Our evaluation system addresses critical gaps in MCP service assessment through a comprehensive quality model, as defined in the main project:

- **Functionality (30%)**: Measures correct implementation of specified requirements
- **Robustness (20%)**: Assesses resilience to invalid inputs and unexpected conditions
- **Security (20%)**: Evaluates vulnerability prevention and data protection
- **Performance (20%)**: Quantifies operational efficiency and resource utilization
- **Transparency (10%)**: Measures clarity and completeness of documentation

## Usage Methods

The testing system supports multiple testing modes through `main.py`. Each mode is designed for different testing scenarios:

### 1. Single Server Testing (Most Common)

Use `intelligent_benchmark.py` to directly test a single MCP server:

```bash
python intelligent_benchmark.py path/to/server.py
```

This will:

- Analyze all tools provided by the server
- Intelligently generate test cases
- Execute tests and generate detailed reports

### 2. Batch Testing Modes

#### Public Servers Testing

Test all public MCP servers in the specified directory:

```bash
python main.py --mode public --public-servers-dir ../workspace/public-mcp-servers/
```

#### MetaGPT Servers Testing

Test MetaGPT framework generated servers:

```bash
python main.py --mode metagpt --metagpt-servers-dir ../workspace/metaGPT-servers/
```

#### Pipeline Model Servers Testing

Test servers generated by different AI models (DeepSeek V3, GPT-4o, etc.):

```bash
python main.py --mode pipeline --pipeline-mapping-file ../workspace/pipeline_mapping.json
```

### 4. Log Analysis Modes

#### Agent Log Analysis

Analyze general agent interaction logs:

```bash
python main.py --mode log --log-dir ../logs/agent_logs/
```

#### Benchmark Log Analysis

Analyze Benchmark testing logs independently:

```bash
python main.py --mode analyze-benchmark --log-dir ../logs/agent_logs/
```

This will analyze the latest Benchmark log files and generate dedicated reports.

### 5. Advanced Configuration Options

#### Concurrency Control

Control the number of concurrent tests (default is 1):

```bash
python main.py --mode public --concurrency 4 --public-servers-dir ../workspace/public-mcp-servers/
```

#### Custom Output Directory

Specify custom output directory for reports:

```bash
python main.py --mode pipeline \
    --pipeline-mapping-file ../workspace/pipeline_mapping.json \
    --output-dir ./custom_reports
```

#### Custom Log Directory

Specify custom log directory:

```bash
python main.py --mode log --log-dir ./custom_logs --output-dir ./custom_reports
```
