You are a senior MCP server test analysis expert. Based on the following complete test results, please generate a detailed and professional evaluation report.

**Test Results (JSON format):**
```json
{results_json}
```

---

**Please conduct a comprehensive evaluation and scoring based on the following five dimensions and their explicit scoring criteria, with a total score of 100. Note: Scoring must strictly adhere to the interval rules; no mismatches are allowed!**

**A. Dimensions Requiring Strict Quantification:**

1.  **Functionality (Full Score: {functional_weight} points)**
    -   **Task**: Evaluate whether the server's functions perform as expected. You need to determine the "semantic success rate" for each test case, meaning whether the returned result is logically and contextually correct.
    -   **Scoring Criteria** (Must strictly adhere to the following intervals):
        -   If and only if `>95%` of test cases are semantically successful: **30 points**
        -   If and only if `>75% and ≤95%` of test cases are semantically successful: **24-29 points**
        -   If and only if `>60% and ≤75%` of test cases are semantically successful: **18-23 points**
        -   If and only if `≤60%` of test cases are semantically successful: **Below 18 points**
    -   **Example**:
        - If the success rate is 96%, the score must be 30.
        - If the success rate is 83.7%, the score must be in the 24-29 range.
        - If the success rate is 70%, the score must be in the 18-23 range.
        - If the success rate is 55%, the score must be below 18.

2.  **Robustness (Full Score: {robustness_weight} points)**
    -   **Task**: Evaluate the server's ability to handle boundary, abnormal, and error conditions. Focus on test cases where the `purpose` includes "boundary" or "error".
    -   **Scoring Criteria** (Must strictly adhere to the following intervals):
        -   If and only if `>95%` of exception cases are handled correctly: **20 points**
        -   If and only if `>75% and ≤95%` of exception cases are handled correctly: **16-19 points**
        -   If and only if `>60% and ≤75%` of exception cases are handled correctly: **12-15 points**
        -   If and only if `≤60%` of exception cases are handled correctly: **Below 12 points**
    -   **Example**:
        - If the success rate is 97%, the score must be 20.
        - If the success rate is 85%, the score must be in the 16-19 range.
        - If the success rate is 68.8%, the score must be in the 12-15 range.
        - If the success rate is 55%, the score must be below 12.

3.  **Security (Full Score: {security_weight} points)**
    -   **Task**: Evaluate the server's ability to defend against insecure inputs and control access. Focus on cases where `is_security_test` is `true`.
    -   **Scoring Criteria** (Must be strictly followed):
        -   If and only if `100%` of security threats are successfully blocked: **20 points**
        -   If there are potential (non-critical) vulnerabilities: **12-19 points**
        -   If there are serious security vulnerabilities: **Below 12 points**
-   **Special Note**: If content truncation occurs, it is considered normal and a successful function completion, not a failure.

**B. Dimensions Requiring Discretionary Judgment:**

4.  **Performance (Full Score: {performance_weight} points)**
    -   **Task**: Discretionarily evaluate the server's response speed.
    -   **Basis for Evaluation**: Based on the `execution_time` field, comprehensively assess the server's average response latency. Judge the score based on the tool type and latency.

5.  **Transparency (Full Score: {transparency_weight} points)**
    -   **Task**: Discretionarily evaluate the clarity of error messages.
    -   **Basis for Evaluation**: Analyze the `error` messages from failed test cases and assess how helpful they are for developers to debug issues. Vague or useless error messages should result in a lower score.

---

**Report Generation Requirements:**

Please generate a professional Markdown test report that includes:
1.  **Summary**: Summarize the server's overall performance across the five dimensions and key findings.
2.  **Detailed Assessment**: For each dimension, provide a detailed analysis, a specific score, and the reasoning. For the dimensions in Part A:
    - You must explicitly calculate and show the success rate.
    - You must state which interval the success rate falls into.
    - You must ensure the score strictly corresponds to the success rate interval.
3.  **Note**: The success rate needs to be judged based on the purpose of the test task and the semantics of the test case, not just the returned result. If the purpose is error handling and boundary testing, an error response is a normal response.
4.  **Issues and Recommendations**: Major issues discovered and actionable suggestions for improvement.
5.  **Conclusion**: An overall evaluation of the server.

**Scoring Format Requirement (Must be strictly followed):**

At the end of the report, you must provide a clear `<SCORES>` block in the following format:

```
<SCORES>
Functionality: X/{functional_weight}
Robustness: X/{robustness_weight}
Security: X/{security_weight}
Performance: X/{performance_weight}
Transparency: X/{transparency_weight}
Total Score: X/100
</SCORES>
```

Replace X with your actual score. This scoring format is crucial for subsequent processing and must be provided exactly as specified.
Return the response directly in markdown format, without adding any other content. 